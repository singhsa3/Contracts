{"cells":[{"cell_type":"markdown","metadata":{"id":"N2vFmKXiVQOB"},"source":["**How this will work** <br>\n","\n","\n","1.   A user will provide a hypothesis and a document and the model will then infer if there is any clause in the document that support, contradict or not mentioned(neutral) the hypothesis\n","2.   Model will be batch trained. Each batch will represent one document and one hypothesis. \n","> Note that in a document there can be 100's of clauses but just one or two clause may support or contradict the hypothesis. This implies over 90% of clauses will be neutral to the hypothesis. This makes data highly imbalanced <br>\n","> I have a parameter max_neutral that controles how many neutral cases we want to add\n","\n","\n","\n","\n"],"id":"N2vFmKXiVQOB"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21782,"status":"ok","timestamp":1668570053665,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"},"user_tz":300},"id":"1hXmxlsgVMgm","outputId":"6eed2c90-a8e0-4c12-f957-4817d402509c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"1hXmxlsgVMgm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqTgjCZVqrg-"},"outputs":[],"source":["import os\n","dir= \"/content/drive/MyDrive/GT/Deep_Learning/Project/contract-nli/contract-nli/\"\n","os.chdir(dir)"],"id":"gqTgjCZVqrg-"},{"cell_type":"code","source":["!apt-get -qq install -y libfluidsynth1\n","!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n","!apt-get -qq install -y graphviz && pip install pydot\n","#!pip install cartopy\n","!apt-get install libomp-dev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMa5PmqzfEkH","executionInfo":{"status":"ok","timestamp":1668570088184,"user_tz":300,"elapsed":31626,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"09dfccc4-6d7b-4c70-d73d-2b11a0e90b5b"},"id":"FMa5PmqzfEkH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package libfluidsynth1:amd64.\n","(Reading database ... 123991 files and directories currently installed.)\n","Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n","Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n","Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n","Selecting previously unselected package libarchive-dev:amd64.\n","(Reading database ... 123996 files and directories currently installed.)\n","Preparing to unpack .../libarchive-dev_3.2.2-3.1ubuntu0.7_amd64.deb ...\n","Unpacking libarchive-dev:amd64 (3.2.2-3.1ubuntu0.7) ...\n","Setting up libarchive-dev:amd64 (3.2.2-3.1ubuntu0.7) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting libarchive\n","  Downloading libarchive-0.4.7.tar.gz (23 kB)\n","Collecting nose\n","  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n","\u001b[K     |████████████████████████████████| 154 kB 6.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: libarchive\n","  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libarchive: filename=libarchive-0.4.7-py3-none-any.whl size=31646 sha256=60b31f0b99a28ccdc71da9b8ba3c9ced33b02cff83464fb000e3f5270f97ea7b\n","  Stored in directory: /root/.cache/pip/wheels/63/b1/c6/b3da79bec2012175bd43603eed98ef8548ac1733b77c1d4330\n","Successfully built libarchive\n","Installing collected packages: nose, libarchive\n","Successfully installed libarchive-0.4.7 nose-1.3.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (3.0.9)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  libomp5\n","Suggested packages:\n","  libomp-doc\n","The following NEW packages will be installed:\n","  libomp-dev libomp5\n","0 upgraded, 2 newly installed, 0 to remove and 5 not upgraded.\n","Need to get 239 kB of archives.\n","After this operation, 804 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n","Fetched 239 kB in 1s (357 kB/s)\n","Selecting previously unselected package libomp5:amd64.\n","(Reading database ... 124052 files and directories currently installed.)\n","Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n","Unpacking libomp5:amd64 (5.0.1-1) ...\n","Selecting previously unselected package libomp-dev.\n","Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n","Unpacking libomp-dev (5.0.1-1) ...\n","Setting up libomp5:amd64 (5.0.1-1) ...\n","Setting up libomp-dev (5.0.1-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"]}]},{"cell_type":"code","source":["!pip install torch\n","!pip install sentence-transformers\n","!pip install pdf_struct\n","!pip install faiss\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mYR7dxPyg2VR","executionInfo":{"status":"ok","timestamp":1668570258653,"user_tz":300,"elapsed":170490,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"578fb852-90b6-4fba-dc49-d74075aa1c2b"},"id":"mYR7dxPyg2VR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 2.6 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 38.9 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 24.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 41.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=54d11f2012130262ec2d869c583e41be517c5685fc19f08d5824fb4533f46a5f\n","  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","Successfully installed huggingface-hub-0.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pdf_struct\n","  Downloading pdf_struct-0.3.4-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n","\u001b[?25hCollecting pdfminer.six==20220524\n","  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 3.0 MB/s \n","\u001b[?25hCollecting click==8.1.3\n","  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n","\u001b[K     |████████████████████████████████| 96 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from pdf_struct) (0.5.3)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from pdf_struct) (0.38.3)\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.6 kB/s \n","\u001b[?25hCollecting pdf_struct\n","  Downloading pdf_struct-0.3.3-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 56 kB/s \n","\u001b[?25hCollecting pdfminer.six==20200726\n","  Downloading pdfminer.six-20200726-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 24.8 MB/s \n","\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from pdf_struct) (7.1.2)\n","Collecting sklearn==0.0\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Collecting beautifulsoup4==4.9.3\n","  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 44.2 MB/s \n","\u001b[?25hCollecting tqdm==4.48.0\n","  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 41.7 MB/s \n","\u001b[?25hCollecting transformers==4.9.1\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 44.7 MB/s \n","\u001b[?25hCollecting regex==2020.7.14\n","  Downloading regex-2020.7.14-cp37-cp37m-manylinux2010_x86_64.whl (660 kB)\n","\u001b[K     |████████████████████████████████| 660 kB 57.9 MB/s \n","\u001b[?25hCollecting twine\n","  Downloading twine-4.0.1-py3-none-any.whl (36 kB)\n","Collecting joblib==1.0.0\n","  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n","\u001b[K     |████████████████████████████████| 302 kB 37.5 MB/s \n","\u001b[?25hCollecting numpy==1.19.1\n","  Downloading numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n","\u001b[K     |████████████████████████████████| 14.5 MB 4.3 MB/s \n","\u001b[?25hCollecting soupsieve>1.2\n","  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n","Collecting cryptography\n","  Downloading cryptography-38.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 41.9 MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200726->pdf_struct) (2.4.0)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200726->pdf_struct) (3.0.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->pdf_struct) (1.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->pdf_struct) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->pdf_struct) (3.8.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 61.5 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 36.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->pdf_struct) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->pdf_struct) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->pdf_struct) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->pdf_struct) (4.13.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.1->pdf_struct) (3.0.9)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20200726->pdf_struct) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20200726->pdf_struct) (2.21)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.9.1->pdf_struct) (3.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->pdf_struct) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->pdf_struct) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->pdf_struct) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1->pdf_struct) (1.15.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->pdf_struct) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->pdf_struct) (3.1.0)\n","Collecting requests-toolbelt!=0.9.0,>=0.8.0\n","  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n","\u001b[?25hCollecting readme-renderer>=35.0\n","  Downloading readme_renderer-37.3-py3-none-any.whl (14 kB)\n","Collecting twine\n","  Downloading twine-4.0.0-py3-none-any.whl (36 kB)\n","  Downloading twine-3.8.0-py3-none-any.whl (36 kB)\n","Collecting colorama>=0.4.3\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting twine\n","  Downloading twine-3.7.1-py3-none-any.whl (35 kB)\n","Collecting keyring>=15.1\n","  Downloading keyring-23.11.0-py3-none-any.whl (36 kB)\n","Collecting pkginfo>=1.8.1\n","  Downloading pkginfo-1.8.3-py2.py3-none-any.whl (26 kB)\n","Collecting rfc3986>=1.4.0\n","  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n","Collecting jaraco.classes\n","  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n","Collecting SecretStorage>=3.2\n","  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n","Collecting jeepney>=0.4.2\n","  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n","\u001b[K     |████████████████████████████████| 48 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.7/dist-packages (from readme-renderer>=35.0->twine->pdf_struct) (0.17.1)\n","Requirement already satisfied: bleach>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from readme-renderer>=35.0->twine->pdf_struct) (5.0.1)\n","Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from readme-renderer>=35.0->twine->pdf_struct) (2.6.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach>=2.1.0->readme-renderer>=35.0->twine->pdf_struct) (0.5.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from jaraco.classes->keyring>=15.1->twine->pdf_struct) (9.0.0)\n","Building wheels for collected packages: sklearn, sacremoses\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=723bb3284da41593fab60752fca27d706530cd934903b570f4115f01c00f0d13\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=b4a5dc69275f2173956a1369556a2cbdf0f261de87c0ed5379f6385d6c96e14e\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sklearn sacremoses\n","Installing collected packages: numpy, jeepney, cryptography, tqdm, SecretStorage, regex, joblib, jaraco.classes, tokenizers, soupsieve, sacremoses, rfc3986, requests-toolbelt, readme-renderer, pkginfo, keyring, huggingface-hub, colorama, twine, transformers, torch, sklearn, sentencepiece, pdfminer.six, beautifulsoup4, pdf-struct\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.1\n","    Uninstalling tqdm-4.64.1:\n","      Successfully uninstalled tqdm-4.64.1\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2022.6.2\n","    Uninstalling regex-2022.6.2:\n","      Successfully uninstalled regex-2022.6.2\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.2.0\n","    Uninstalling joblib-1.2.0:\n","      Successfully uninstalled joblib-1.2.0\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.2\n","    Uninstalling tokenizers-0.13.2:\n","      Successfully uninstalled tokenizers-0.13.2\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.10.1\n","    Uninstalling huggingface-hub-0.10.1:\n","      Successfully uninstalled huggingface-hub-0.10.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.24.0\n","    Uninstalling transformers-4.24.0:\n","      Successfully uninstalled transformers-4.24.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.97\n","    Uninstalling sentencepiece-0.1.97:\n","      Successfully uninstalled sentencepiece-0.1.97\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.1 which is incompatible.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.19.1 which is incompatible.\n","sentence-transformers 2.2.2 requires huggingface-hub>=0.4.0, but you have huggingface-hub 0.0.12 which is incompatible.\n","nltk 3.7 requires regex>=2021.8.3, but you have regex 2020.7.14 which is incompatible.\n","jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.1 which is incompatible.\n","jax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.1 which is incompatible.\n","cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.1 which is incompatible.\u001b[0m\n","Successfully installed SecretStorage-3.3.3 beautifulsoup4-4.9.3 colorama-0.4.6 cryptography-38.0.3 huggingface-hub-0.0.12 jaraco.classes-3.2.3 jeepney-0.8.0 joblib-1.0.0 keyring-23.11.0 numpy-1.19.1 pdf-struct-0.3.3 pdfminer.six-20200726 pkginfo-1.8.3 readme-renderer-37.3 regex-2020.7.14 requests-toolbelt-0.10.1 rfc3986-2.0.0 sacremoses-0.0.53 sentencepiece-0.1.96 sklearn-0.0 soupsieve-2.3.2.post1 tokenizers-0.10.3 torch-1.9.0 tqdm-4.48.0 transformers-4.9.1 twine-3.7.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting faiss\n","  Downloading faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.1)\n","Installing collected packages: faiss\n","Successfully installed faiss-1.5.3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-XJP1t2tF-y"},"outputs":[],"source":["import json\n","import pickle\n","import pandas as pd\n","import re \n","import transformers\n","import torch\n","import numpy as np\n","#from torchtext.data import get_tokenizer\n","#from transformers import BertTokenizer, BertModel, BertConfig\n","from torch.utils.data import Dataset, DataLoader\n","\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","#logging.basicConfig(level=logging.INFO)\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","from sklearn.metrics import pairwise\n","\n","d=768 # Do not change\n","\n"],"id":"8-XJP1t2tF-y"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2350,"status":"ok","timestamp":1668570279268,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"},"user_tz":300},"id":"mWPAon7tAWrj","outputId":"91119283-69e7-454c-c39d-0bb1e55a5dac"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer,PorterStemmer\n","from nltk.corpus import stopwords\n","nltk.download('omw-1.4')\n","import re\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer() \n","\n","mlngt = 100 #max_length,\n","mn =10 #max_neutral\n","\n","def clean_head(ab):\n","    ab = ab.replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\")\n","    ab = ab.lower().strip()    \n","    #marked_text = \"[CLS] \" + ab + \" [SEP]\"\n","    # Tokenize our sentence with the BERT tokenizer.\n","    #ab = tokenizer.tokenize(marked_text) \n","    return ab\n","\n","def preprocess(sentence):\n","  sentence=str(sentence)\n","  sentence = sentence.lower().replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\").replace(\"__\",\" \").replace(\"  \",\" \")\n","  sentence=sentence.replace('{html}',\"\") \n","  cleanr = re.compile('<.*?>')\n","  cleantext = re.sub(cleanr, '', sentence)\n","  rem_url=re.sub(r'http\\S+', '',cleantext)\n","  rem_num = re.sub('[0-9]+', '', rem_url)\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  tokens = tokenizer.tokenize(rem_num)  \n","  #filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n","  #stem_words=[stemmer.stem(w) for w in filtered_words]\n","  #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n","  return \" \".join(tokens)\n","\n","\n","\n","def getPositionEncoding(max_length, d, n=10000):\n","    P = np.zeros((max_length, d))\n","    #print(max_length,P.shape)\n","    for k in range(max_length):      \n","      for i in np.arange(int(d/2)):\n","        denominator = np.power(n, 2*i/d)\n","        P[k, 2*i] = np.sin(k/denominator)\n","        P[k, 2*i+1] = np.cos(k/denominator)\n","    return P\n","Pg =  getPositionEncoding(mlngt, d)  \n","Pg  = torch.FloatTensor(Pg)\n","\n","def emd(sent, max_length=mlngt):\n","  tok = get_tokenizer(\"basic_english\")\n","  tkn = tok(sent)  \n","  ln = len(tkn)\n","  x = global_vectors.get_vecs_by_tokens(tkn)\n","  #print(x.shape, Pg.shape)\n","  if ln<max_length:  \n","    x = torch.nn.ConstantPad2d((0, 0, 0, max_length-ln), 0)(x)\n","  else:\n","    x= x[:max_length,:]\n","  x=x+Pg \n","  return x\n","\n","def emdb(sent):\n","  # Tokenize sentences\n","  encoded_input = tokenizer(sent, padding=True, truncation=True, return_tensors='pt')\n","\n","  # Compute token embeddings\n","  with torch.no_grad():\n","      model_output = model(**encoded_input)\n","\n","  # Perform pooling. In this case, max pooling.\n","  x = mean_pooling(model_output, encoded_input['attention_mask'])\n","  seg = torch.tensor([0,1])\n","  seg = seg.tile((768,1)).t()    \n","  x=x+seg  \n","  return x\n"],"id":"mWPAon7tAWrj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnzc05BuQWMy"},"outputs":[],"source":["#################\n","# Opening JSON file\n","f = open('train.json')  \n","# returns JSON object as \n","# a dictionary\n","data = json.load(f)\n","flnmidx={}\n","for dcl in range(len(data['documents'])):\n","  flnmidx[data['documents'][dcl]['file_name']]= dcl\n","\n","\n","lbl={}\n","kes =[]\n","for ke in data['labels']:\n","  lbl[ke]= data['labels'][ke]['hypothesis']\n","  kes.append(ke)\n","\n","kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n","rg = len(data['documents'])\n","reference_list=[]\n","idx=0\n","for idx in range(rg):\n","  #Document level\n","  docid = data['documents'][idx]['id']\n","  for ndas_key in kes:\n","    reference_list.append([docid, ndas_key,idx])\n","  idx=idx+1\n","ref_train = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n","f.close()\n","##############\n","# Opening JSON file\n","f = open('test.json')  \n","# returns JSON object as \n","# a dictionary\n","dataT = json.load(f)\n","\n","kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n","rg = len(dataT['documents'])\n","reference_list=[]\n","idxt=0\n","for idxt in range(rg):\n","  #Document level\n","  docid = dataT['documents'][idxt]['id']\n","\n","  for ndas_key in kes:\n","    reference_list.append([docid, ndas_key, idxt])\n","  idxt=idxt+1\n","ref_test = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])"],"id":"wnzc05BuQWMy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYXSVq2As5R5"},"outputs":[],"source":["# Do Not delete\n","def get_data(data, idx,ke, max_neutral=mn):\n","  dataM=[]\n","  #Document level\n","  docid = data['documents'][idx]['id']\n","  \n","  string = data['documents'][idx]['text']\n","  file_name = data['documents'][idx]['file_name']\n","  spans = data['documents'][idx]['spans']\n","  spanall=[]\n","  for span in spans:           \n","      spanval = string[span[0]: span[1]]\n","      spanall.append(spanval)\n","  \n","  ndas = data['documents'][idx]['annotation_sets'][0]['annotations'] \n","  # Keys level\n","    \n","  hypothesis = data['labels'][ke]['hypothesis']  \n","\n","  # Key level   \n","\n","  choice =ndas[ke] ['choice']\n","  spansC = ndas[ke] ['spans']    \n","  for si in range(len(spanall)):\n","    span_nbr =si\n","    if si in spansC:\n","      val=choice\n","    else:\n","      val=\"Neutral\"\n","    premise = spanall[si]\n","    itm = [docid, file_name , ke, hypothesis, span_nbr, premise,val ]\n","    dataM.append(itm)\n","  df = pd.DataFrame(dataM ,columns = ['docid', 'file_name', 'hypotheis_key', 'hypotheis', 'span_nbr', 'premise', 'choice'   ])\n","  #\"[CLS] \" and \" [SEP] \"\" [SEP]\" \n","  df['premise'] = df['premise'].apply(preprocess)\n","  df['hypotheis']= df['hypotheis'].apply(preprocess) \n","  df['label'] = df['choice'].map(lambda s: 0 if s=='Entailment' else (1 if s== 'Contradiction' else 2 ))\n","  df['entl'] =  df['choice'].map(lambda s : 1 if s== 'Entailment' else 0)\n","  df['cont'] =  df['choice'].map(lambda s : 1 if s== 'Contradiction' else 0)\n","  df['neut'] =  df['choice'].map(lambda s : 1 if s== 'Neutral' else 0)\n","  \n","  df1=df[df.choice !='Neutral']\n","  df2=df[df.choice =='Neutral']\n","  n = min(max_neutral, len(df2)-1)\n","  df2= df2.sample(n = n)\n","  df = pd.concat([df1, df2], sort=False)\n","  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n","  df['bert_sent']= df.apply(lambda s: [s.premise,s.hypotheis], axis=1)\n","  return df\n","\n","def create_completedf(data, refdf, max_neutral=50):\n","  cl = get_data(data, 0,'nda-1', 1)\n","  cols = list(cl.columns)\n","  df1= pd.DataFrame(columns=cols)\n","  for ind, row in refdf.iterrows():    \n","    df2 = get_data(data, row[\"idx\"],row[\"nda_key\"], max_neutral)\n","    df1=pd.concat([df1,df2])\n","  return df1\n","\n"],"id":"hYXSVq2As5R5"},{"cell_type":"code","source":["#https://gist.github.com/jamescalam/7117aa92235a7f52141ad0654795aa48\n","\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","def sent_embs(file, json, ke, premise=True):\n","  idx= flnmidx[file]\n","  ad =get_data(json, idx,ke, max_neutral=5000)\n","  if premise:\n","    sentences = ad.premise.values.tolist()\n","    '''\n","    spans= pdf_struct.predict(\n","                        format='paragraphs',\n","                        in_path=x[0],\n","                        model='PDFContractEnFeatureExtractor'\n","                      )\n","    sentences = pd.DataFrame(spans)[0].apply(preprocess).tolist()\n","    '''\n","  else:\n","    sentences = [ad[ad.hypotheis_key==ke].hypotheis.values[0]]\n","\n","  # Load model from HuggingFace Hub\n","  tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/nli-bert-base')\n","  model = AutoModel.from_pretrained('sentence-transformers/nli-bert-base')\n","\n","  # Tokenize sentences\n","  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n","\n","  # Compute token embeddings\n","  with torch.no_grad():\n","      model_output = model(**encoded_input)\n","\n","  # Perform pooling. In this case, max pooling.\n","  embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","  return sentences, embeddings"],"metadata":{"id":"hlhurhVmynRB"},"id":"hlhurhVmynRB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n","idx= 25\n","ke= 'nda-4'\n","ad =get_data(data, idx,ke, max_neutral=3)\n","ad.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"DWJis_8ojhR0","executionInfo":{"status":"ok","timestamp":1668576864102,"user_tz":300,"elapsed":213,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"7bd3fc70-d5bb-4076-8cca-9a4b7775d75e"},"id":"DWJis_8ojhR0","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"]},{"output_type":"execute_result","data":{"text/plain":["     docid                               file_name hypotheis_key  \\\n","40     114  GSAOP0309-Non-disclosure-agreement.pdf         nda-4   \n","52     114  GSAOP0309-Non-disclosure-agreement.pdf         nda-4   \n","53     114  GSAOP0309-Non-disclosure-agreement.pdf         nda-4   \n","157    114  GSAOP0309-Non-disclosure-agreement.pdf         nda-4   \n","148    114  GSAOP0309-Non-disclosure-agreement.pdf         nda-4   \n","\n","                                             hypotheis  span_nbr  \\\n","40   receiving party shall not use any confidential...        40   \n","52   receiving party shall not use any confidential...        52   \n","53   receiving party shall not use any confidential...        53   \n","157  receiving party shall not use any confidential...       157   \n","148  receiving party shall not use any confidential...       148   \n","\n","                                               premise      choice  label  \\\n","40   under this agreement the recipient undertakes ...  Entailment      0   \n","52   the confidential information is supplied to th...  Entailment      0   \n","53   the confidential information cannot be used to...  Entailment      0   \n","157  this agreement and the rights and obligations ...  Entailment      0   \n","148  this agreement shall be governed and construed...     Neutral      2   \n","\n","     entl  cont  neut                                          bert_sent  \n","40      1     0     0  [under this agreement the recipient undertakes...  \n","52      1     0     0  [the confidential information is supplied to t...  \n","53      1     0     0  [the confidential information cannot be used t...  \n","157     1     0     0  [this agreement and the rights and obligations...  \n","148     0     0     1  [this agreement shall be governed and construe...  "],"text/html":["\n","  <div id=\"df-285a299e-49d6-444b-9412-dced90150af1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>docid</th>\n","      <th>file_name</th>\n","      <th>hypotheis_key</th>\n","      <th>hypotheis</th>\n","      <th>span_nbr</th>\n","      <th>premise</th>\n","      <th>choice</th>\n","      <th>label</th>\n","      <th>entl</th>\n","      <th>cont</th>\n","      <th>neut</th>\n","      <th>bert_sent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>40</th>\n","      <td>114</td>\n","      <td>GSAOP0309-Non-disclosure-agreement.pdf</td>\n","      <td>nda-4</td>\n","      <td>receiving party shall not use any confidential...</td>\n","      <td>40</td>\n","      <td>under this agreement the recipient undertakes ...</td>\n","      <td>Entailment</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[under this agreement the recipient undertakes...</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>114</td>\n","      <td>GSAOP0309-Non-disclosure-agreement.pdf</td>\n","      <td>nda-4</td>\n","      <td>receiving party shall not use any confidential...</td>\n","      <td>52</td>\n","      <td>the confidential information is supplied to th...</td>\n","      <td>Entailment</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[the confidential information is supplied to t...</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>114</td>\n","      <td>GSAOP0309-Non-disclosure-agreement.pdf</td>\n","      <td>nda-4</td>\n","      <td>receiving party shall not use any confidential...</td>\n","      <td>53</td>\n","      <td>the confidential information cannot be used to...</td>\n","      <td>Entailment</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[the confidential information cannot be used t...</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>114</td>\n","      <td>GSAOP0309-Non-disclosure-agreement.pdf</td>\n","      <td>nda-4</td>\n","      <td>receiving party shall not use any confidential...</td>\n","      <td>157</td>\n","      <td>this agreement and the rights and obligations ...</td>\n","      <td>Entailment</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[this agreement and the rights and obligations...</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>114</td>\n","      <td>GSAOP0309-Non-disclosure-agreement.pdf</td>\n","      <td>nda-4</td>\n","      <td>receiving party shall not use any confidential...</td>\n","      <td>148</td>\n","      <td>this agreement shall be governed and construed...</td>\n","      <td>Neutral</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>[this agreement shall be governed and construe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-285a299e-49d6-444b-9412-dced90150af1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-285a299e-49d6-444b-9412-dced90150af1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-285a299e-49d6-444b-9412-dced90150af1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"zJdrrg5s8RA3","executionInfo":{"status":"ok","timestamp":1668577045947,"user_tz":300,"elapsed":168,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"d6a15ecd-7f23-4fb6-c57f-1ae153dd0181"},"id":"zJdrrg5s8RA3","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Receiving Party shall not use any Confidential Information for any purpose other than the purposes stated in Agreement.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["x= data['documents'][idx]['file_name']\n","x"],"metadata":{"id":"FE0S6WYbufRh","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1668576873900,"user_tz":300,"elapsed":209,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"9ccf4faf-afa7-48fc-a9cc-fbefe6f9b065"},"id":"FE0S6WYbufRh","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'GSAOP0309-Non-disclosure-agreement.pdf'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["import faiss\n","import pdf_struct\n","index = faiss.IndexFlatL2(d)\n","\n","sentences_contract , contract_embeddings = sent_embs(x,data,ke)\n","index.add(contract_embeddings.numpy())\n","sentences_hypt, xq = sent_embs(x,data,ke, premise=False)\n","print(len(sentences_contract))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOkA6YSRVAz-","executionInfo":{"status":"ok","timestamp":1668576930708,"user_tz":300,"elapsed":55203,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"b990b160-00b1-4d3e-cdd9-28b29c064918"},"id":"zOkA6YSRVAz-","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["173\n"]}]},{"cell_type":"code","source":["k = 15\n","D, I = index.search(xq.numpy(), k)  # search\n","print(I) \n","hits=[f'{sentences_contract[i]}' for i in I[0]]\n","#hits = [f'{i}: {sentences_contract[i]}' for i in I[0]]\n","\n","hits\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgYVoL0cT2Vu","executionInfo":{"status":"ok","timestamp":1668576930709,"user_tz":300,"elapsed":46,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"86afd9f3-7b55-4696-b618-6debb4bbea23"},"id":"lgYVoL0cT2Vu","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[127 149   5 108 161 115  92 158  48  70   3 165   2 151 145]]\n"]},{"output_type":"execute_result","data":{"text/plain":["['no failure or delay by the gsa in exercising any of its rights under this agreement shall operate as a waiver thereof nor shall any single or partial exercise preclude any other or further exercise of such rights',\n"," 'no provision of this agreement shall be construed to be an obligation by either party to disclose information to the other party or to enter into further agreements with the other',\n"," 'in any case the recipient and authorised third parties shall not use the confidential information',\n"," 'the recipient shall not itself nor authorise authorised third party or any third party to write publish or disseminate any description of the confidential information or elements of it such as its structure or content for so long as it is bound by this agreement',\n"," 'paragraphs and above shall not apply to the extent that the recipient is required to retain any such confidential information by any applicable law rule or regulation or by any competent judicial or governmental body',\n"," 'nothing contained in this agreement shall be construed as granting any right title or interest in the confidential information including any intellectual property right',\n"," 'no amendment or modification of this agreement shall be binding or effective unless made in writing and signed on behalf of both parties by their respective duly authorised representative',\n"," 'if any term of this agreement is or becomes illegal invalid or unenforceable in any jurisdiction that shall not affect the legality validity or enforceability in that jurisdiction of any other terms of this agreement nor the legality validity or enforceability in other jurisdictions of that or any other provision of this agreement',\n"," 'for the purpose of this agreement the expression confidential information does not make reference to the eu classification of documents',\n"," 'the recipient shall not copy reproduce duplicate distribute communicate or otherwise make available the confidential information either in whole or in part to persons or parties who are not authorised third parties unless the gsa gives its prior written authorisation',\n"," 'this agreement and the rights and obligations hereunder may not be transferred or assigned by the recipient without the prior written approval of the gsa',\n"," 'was lawfully obtained by the recipient without restriction and without breach of this agreement from a third party who is in lawful possession thereof and under no obligation of confidence to the gsa',\n"," 'the confidential information cannot be used totally or partially directly or indirectly for any other purpose than that defined in article above unless the gsa gives its prior written authorisation',\n"," 'the gsa will not be liable for any damages whatsoever including but not limited to damages for loss of business profit business interruption loss of business information or any other pecuniary loss arising out of the use of or inability to use the confidential information',\n"," 'has been or is published without violation of this agreement']"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","source":["print(lbl[ke])\n","for pm in ad[ad.choice !='Neutral'][\"premise\"].values:\n","  print(\"I am looking for : \",pm)\n","  i=0\n","  for sc in hits:\n","    if pm==sc:      \n","      print (i, I[0][i], sc)\n","    i=i+1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezJXzidUraAv","executionInfo":{"status":"ok","timestamp":1668577065094,"user_tz":300,"elapsed":225,"user":{"displayName":"Sanjeev Singh Kenwar","userId":"00714071671360104547"}},"outputId":"0d2247a9-c105-496e-a684-aedd8acead3c"},"id":"ezJXzidUraAv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Receiving Party shall not use any Confidential Information for any purpose other than the purposes stated in Agreement.\n","I am looking for :  under this agreement the recipient undertakes to use the confidential information solely for pursuing the purpose in accordance with the terms of this agreement\n","I am looking for :  the confidential information is supplied to the recipient solely and exclusively for the purpose\n","I am looking for :  the confidential information cannot be used totally or partially directly or indirectly for any other purpose than that defined in article above unless the gsa gives its prior written authorisation\n","12 2 the confidential information cannot be used totally or partially directly or indirectly for any other purpose than that defined in article above unless the gsa gives its prior written authorisation\n","I am looking for :  this agreement and the rights and obligations hereunder may not be transferred or assigned by the recipient without the prior written approval of the gsa\n","10 3 this agreement and the rights and obligations hereunder may not be transferred or assigned by the recipient without the prior written approval of the gsa\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}