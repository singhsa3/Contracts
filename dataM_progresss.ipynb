{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**How this will work** <br>\n",
        "\n",
        "\n",
        "1.   A user will provide a hypothesis and a document and the model will then infer if there is any clause in the document that support, contradict or not mentioned(neutral) the hypothesis\n",
        "2.   Model will be batch trained. Each batch will represent one document and one hypothesis. \n",
        "> Note that in a document there can be 100's of clauses but just one or two clause may support or contradict the hypothesis. This implies over 90% of clauses will be neutral to the hypothesis. This makes data highly imbalanced <br>\n",
        "> I have a parameter max_neutral that controles how many neutral cases we want to add\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2vFmKXiVQOB"
      },
      "id": "N2vFmKXiVQOB"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXmxlsgVMgm",
        "outputId": "acfed1aa-ea12-4a7b-ba35-7bf32cc6532e"
      },
      "id": "1hXmxlsgVMgm",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dir= \"/content/drive/MyDrive/Github/CS-7643/project/contract-nli\"\n",
        "os.chdir(dir)"
      ],
      "metadata": {
        "id": "gqTgjCZVqrg-"
      },
      "id": "gqTgjCZVqrg-",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert-pytorch\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install onnx\n",
        "!pip install fasttext\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "# https://huggingface.co/docs/transformers/preprocessing"
      ],
      "metadata": {
        "id": "-Zn1oKifzYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677fcc72-1576-4c33-b13d-058c40d0ef4f"
      },
      "id": "-Zn1oKifzYiQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 63.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.19.6)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3157960 sha256=c2701a72c378e2b2bd7d88f017f79620d1508deb77213f464a48463e725a3d80\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re \n",
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchtext.data import get_tokenizer\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise"
      ],
      "metadata": {
        "id": "8-XJP1t2tF-y"
      },
      "id": "8-XJP1t2tF-y",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take time\n",
        "d=100\n",
        "from torchtext.vocab import GloVe\n",
        "#global_vectors = GloVe(name='840B', dim=d)\n",
        "global_vectors = GloVe(name='6B', dim=d)"
      ],
      "metadata": {
        "id": "t3mKE1z8GDQp"
      },
      "id": "t3mKE1z8GDQp",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('omw-1.4')\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "mlngt = 100 #max_length,\n",
        "mn =10 #max_neutral\n",
        "\n",
        "def clean_head(ab):\n",
        "    ab = ab.replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\")\n",
        "    ab = ab.lower().strip()    \n",
        "    #marked_text = \"[CLS] \" + ab + \" [SEP]\"\n",
        "    # Tokenize our sentence with the BERT tokenizer.\n",
        "    #ab = tokenizer.tokenize(marked_text) \n",
        "    return ab\n",
        "\n",
        "def preprocess(sentence):\n",
        "  sentence=str(sentence)\n",
        "  sentence = sentence.lower().replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\").replace(\"__\",\" \").replace(\"  \",\" \")\n",
        "  sentence=sentence.replace('{html}',\"\") \n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', sentence)\n",
        "  rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "  rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(rem_num)  \n",
        "  #filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "  #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "  #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def getPositionEncoding(max_length, d, n=10000):\n",
        "    P = np.zeros((max_length, d))\n",
        "    #print(max_length,P.shape)\n",
        "    for k in range(max_length):      \n",
        "      for i in np.arange(int(d/2)):\n",
        "        denominator = np.power(n, 2*i/d)\n",
        "        P[k, 2*i] = np.sin(k/denominator)\n",
        "        P[k, 2*i+1] = np.cos(k/denominator)\n",
        "    return P\n",
        "Pg =  getPositionEncoding(mlngt, d)  \n",
        "Pg  = torch.FloatTensor(Pg)\n",
        "\n",
        "def emd(sent, max_length=mlngt):\n",
        "  tok = get_tokenizer(\"basic_english\")\n",
        "  tkn = tok(sent)  \n",
        "  ln = len(tkn)\n",
        "  x = global_vectors.get_vecs_by_tokens(tkn)\n",
        "  #print(x.shape, Pg.shape)\n",
        "  if ln<max_length:  \n",
        "    x = torch.nn.ConstantPad2d((0, 0, 0, max_length-ln), 0)(x)\n",
        "  else:\n",
        "    x= x[:max_length,:]\n",
        "  x=x+Pg \n",
        "  return x\n",
        "\n",
        "def emdb(sent):\n",
        "  # Tokenize sentences\n",
        "  encoded_input = tokenizer(sent, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "  # Compute token embeddings\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "\n",
        "  # Perform pooling. In this case, max pooling.\n",
        "  x = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "  seg = torch.tensor([0,1])\n",
        "  seg = seg.tile((768,1)).t()    \n",
        "  x=x+seg  \n",
        "  return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWPAon7tAWrj",
        "outputId": "e501d05f-9a1e-477f-fb14-75c9db93c016"
      },
      "id": "mWPAon7tAWrj",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ab = emd(\"we are the world\")\n",
        "ab.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llgdXTqnxxyE",
        "outputId": "fce0b592-e2bf-4fca-f269-bbf4ed370d8b"
      },
      "id": "llgdXTqnxxyE",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "# Opening JSON file\n",
        "f = open('train.json')  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n",
        "rg = len(data['documents'])\n",
        "reference_list=[]\n",
        "idx=0\n",
        "for idx in range(rg):\n",
        "  #Document level\n",
        "  docid = data['documents'][idx]['id']\n",
        "  for ndas_key in kes:\n",
        "    reference_list.append([docid, ndas_key,idx])\n",
        "  idx=idx+1\n",
        "ref_train = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n",
        "f.close()\n",
        "##############\n",
        "# Opening JSON file\n",
        "f = open('test.json')  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "dataT = json.load(f)\n",
        "\n",
        "kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n",
        "rg = len(dataT['documents'])\n",
        "reference_list=[]\n",
        "idxt=0\n",
        "for idxt in range(rg):\n",
        "  #Document level\n",
        "  docid = dataT['documents'][idxt]['id']\n",
        "\n",
        "  for ndas_key in kes:\n",
        "    reference_list.append([docid, ndas_key, idxt])\n",
        "  idxt=idxt+1\n",
        "ref_test = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])"
      ],
      "metadata": {
        "id": "wnzc05BuQWMy"
      },
      "id": "wnzc05BuQWMy",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Not delete\n",
        "def get_data(data, idx,ke, max_neutral=mn):\n",
        "  dataM=[]\n",
        "  #Document level\n",
        "  docid = data['documents'][idx]['id']\n",
        "  \n",
        "  string = data['documents'][idx]['text']\n",
        "  file_name = data['documents'][idx]['file_name']\n",
        "  spans = data['documents'][idx]['spans']\n",
        "  spanall=[]\n",
        "  for span in spans:           \n",
        "      spanval = string[span[0]: span[1]]\n",
        "      spanall.append(spanval)\n",
        "  \n",
        "  ndas = data['documents'][idx]['annotation_sets'][0]['annotations'] \n",
        "  # Keys level\n",
        "    \n",
        "  hypothesis = data['labels'][ke]['hypothesis']  \n",
        "\n",
        "  # Key level   \n",
        "\n",
        "  choice =ndas[ke] ['choice']\n",
        "  spansC = ndas[ke] ['spans']    \n",
        "  for si in range(len(spanall)):\n",
        "    span_nbr =si\n",
        "    if si in spansC:\n",
        "      val=choice\n",
        "    else:\n",
        "      val=\"Neutral\"\n",
        "    premise = spanall[si]\n",
        "    itm = [docid, file_name , ke, hypothesis, span_nbr, premise,val ]\n",
        "    dataM.append(itm)\n",
        "  df = pd.DataFrame(dataM ,columns = ['docid', 'file_name', 'hypotheis_key', 'hypotheis', 'span_nbr', 'premise', 'choice'   ])\n",
        "  #\"[CLS] \" and \" [SEP] \"\" [SEP]\" \n",
        "  df['premise'] = df['premise'].apply(preprocess)\n",
        "  df['hypotheis']= df['hypotheis'].apply(preprocess) \n",
        "  df['label'] = df['choice'].map(lambda s: 0 if s=='Entailment' else (1 if s== 'Contradiction' else 2 ))\n",
        "  df['entl'] =  df['choice'].map(lambda s : 1 if s== 'Entailment' else 0)\n",
        "  df['cont'] =  df['choice'].map(lambda s : 1 if s== 'Contradiction' else 0)\n",
        "  df['neut'] =  df['choice'].map(lambda s : 1 if s== 'Neutral' else 0)\n",
        "  \n",
        "  df1=df[df.choice !='Neutral']\n",
        "  df2=df[df.choice =='Neutral']\n",
        "  n = min(max_neutral, len(df2)-1)\n",
        "  df2= df2.sample(n = n)\n",
        "  df = pd.concat([df1, df2], sort=False)\n",
        "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n",
        "  df['bert_sent']= df.apply(lambda s: [s.premise,s.hypotheis], axis=1)\n",
        "  return df\n",
        "\n",
        "def create_completedf(data, refdf, max_neutral=50):\n",
        "  cl = get_data(data, 0,'nda-1', 1)\n",
        "  cols = list(cl.columns)\n",
        "  df1= pd.DataFrame(columns=cols)\n",
        "  for ind, row in refdf.iterrows():    \n",
        "    df2 = get_data(data, row[\"idx\"],row[\"nda_key\"], max_neutral)\n",
        "    df1=pd.concat([df1,df2])\n",
        "  return df1\n",
        "\n"
      ],
      "metadata": {
        "id": "hYXSVq2As5R5"
      },
      "id": "hYXSVq2As5R5",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ref_test, ref_valid = train_test_split(ref_test, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "H2spD4CZ6Cbj"
      },
      "id": "H2spD4CZ6Cbj",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "DUzW-j3QyEJa"
      },
      "id": "DUzW-j3QyEJa",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset): \n",
        "  def __init__(self,data, df, max_neutral=mn):       \n",
        "    self.df =df.sample(frac=1)\n",
        "    self.data =data\n",
        "    self.max_neutral= max_neutral\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    # Is should have used docid but I am using index in json. Check how ref files are structured\n",
        "    docidx = self.df.iloc[idx]['idx']\n",
        "    ndakey = self.df.iloc[idx]['nda_key']\n",
        "    df1 = get_data(data, docidx, ndakey,self.max_neutral)\n",
        "    label = torch.tensor(df1.label.values)\n",
        "    premise = torch.stack(df1.premise.apply(emd).tolist())\n",
        "    hypothesis =  torch.stack(df1.hypotheis.apply(emd).tolist())\n",
        "    #bertsent  = torch.stack(df1.bert_sent.apply(emdb).tolist())\n",
        "    \n",
        "    #sample = {\"Label\": label.to(device), \"Premise\": premise.to(device) , \"Hypothesis\" : hypothesis.to(device), \"Bert_Sent\": bertsent.to(device)}\n",
        "    sample = {\"Label\": label.to(device), \"Premise\": premise.to(device) , \"Hypothesis\" : hypothesis.to(device)}\n",
        "    return sample\n",
        "\n",
        "# Custome collate function\n",
        "def coll(sample1):  \n",
        "  if len(sample1)<2:\n",
        "    return sample1\n",
        "  else:\n",
        "    label0 = sample1[0]['Label'].to(device)\n",
        "    premise0 = sample1[0]['Premise'].to(device)\n",
        "    hypothesis0 = sample1[0]['Hypothesis'].to(device)\n",
        "    #bertsent0 = sample1[0][\"Bert_Sent\"].to(device)\n",
        "    for i in range(1, len(sample1)):\n",
        "      labeli = sample1[i]['Label'].to(device)\n",
        "      premisei = sample1[i]['Premise'].to(device)\n",
        "      hypothesisi = sample1[i]['Hypothesis'].to(device)\n",
        "      #bertsenti = sample1[i][\"Bert_Sent\"].to(device)\n",
        "      label0 =torch.cat((label0,labeli))\n",
        "      premise0 = torch.cat((premise0, premisei))\n",
        "      hypothesis0 = torch.cat((hypothesis0,hypothesisi))\n",
        "      #bertsent0 = torch.cat((bertsent0, bertsenti))\n",
        "    #sample1 = {\"Label\": label0, \"Premise\": premise0 , \"Hypothesis\" : hypothesis0, \"Bert_Sent\": bertsent0}\n",
        "    sample1 = {\"Label\": label0, \"Premise\": premise0 , \"Hypothesis\" : hypothesis0}\n",
        "    return sample1\n",
        "    "
      ],
      "metadata": {
        "id": "qoI6bAQbFCgp"
      },
      "id": "qoI6bAQbFCgp",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=MyDataset(data, ref_train)\n",
        "train_loader=DataLoader(train_data,batch_size=20,collate_fn=coll, shuffle=False)\n",
        "valid_data=MyDataset(dataT, ref_valid,max_neutral=2)\n",
        "valid_loader=DataLoader(valid_data,batch_size=20,collate_fn=coll, shuffle=True)\n",
        "test_data=MyDataset(dataT, ref_test,max_neutral=2)\n",
        "test_loader=DataLoader(test_data,batch_size=20,collate_fn=coll, shuffle=True)\n"
      ],
      "metadata": {
        "id": "m-FgrJXnbTAb"
      },
      "id": "m-FgrJXnbTAb",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x['Label'].shape,x['Premise'].shape,x['Hypothesis'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8NRwBAL8yEX",
        "outputId": "029e434a-d160-4d09-8d3a-afe422b6463f"
      },
      "id": "m8NRwBAL8yEX",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([217]) torch.Size([217, 100, 100]) torch.Size([217, 100, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG_pOaUwdv1m",
        "outputId": "6790ca7c-dce3-4b30-b1ae-d26a3c1d1761"
      },
      "id": "AG_pOaUwdv1m",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: d2l==1.0.0-alpha1.post0 in /usr/local/lib/python3.7/dist-packages (1.0.0a1.post0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.1.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.10.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.4.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.7.16)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.0.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.15.0)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.19.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.6)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "xUTUoh3GcgPK"
      },
      "id": "xUTUoh3GcgPK",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(num_inputs, num_hiddens, flatten):\n",
        "    net = []\n",
        "    net.append(nn.Dropout(0.2))\n",
        "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
        "    net.append(nn.ReLU())\n",
        "    if flatten:\n",
        "        net.append(nn.Flatten(start_dim=1))\n",
        "    net.append(nn.Dropout(0.2))\n",
        "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
        "    net.append(nn.ReLU())\n",
        "    if flatten:\n",
        "        net.append(nn.Flatten(start_dim=1))\n",
        "    return nn.Sequential(*net)"
      ],
      "metadata": {
        "id": "0O1ZiCzfd0g4"
      },
      "id": "0O1ZiCzfd0g4",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attend(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
        "        super(Attend, self).__init__(**kwargs)\n",
        "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
        "\n",
        "    def forward(self, A, B):\n",
        "        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n",
        "        # `embed_size`)\n",
        "        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n",
        "        # `num_hiddens`)\n",
        "        f_A = self.f(A)\n",
        "        f_B = self.f(B)\n",
        "        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n",
        "        # no. of tokens in sequence B)\n",
        "        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
        "        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n",
        "        # `embed_size`), where sequence B is softly aligned with each token\n",
        "        # (axis 1 of `beta`) in sequence A\n",
        "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
        "        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n",
        "        # `embed_size`), where sequence A is softly aligned with each token\n",
        "        # (axis 1 of `alpha`) in sequence B\n",
        "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
        "        return beta, alpha"
      ],
      "metadata": {
        "id": "j_t0nss-d8KO"
      },
      "id": "j_t0nss-d8KO",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Compare(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
        "        super(Compare, self).__init__(**kwargs)\n",
        "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
        "\n",
        "    def forward(self, A, B, beta, alpha):\n",
        "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
        "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
        "        return V_A, V_B"
      ],
      "metadata": {
        "id": "64Iv0sxeeCA-"
      },
      "id": "64Iv0sxeeCA-",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Aggregate(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
        "        super(Aggregate, self).__init__(**kwargs)\n",
        "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
        "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
        "\n",
        "    def forward(self, V_A, V_B):\n",
        "        # Sum up both sets of comparison vectors\n",
        "        V_A = V_A.sum(dim=1)\n",
        "        V_B = V_B.sum(dim=1)\n",
        "        # Feed the concatenation of both summarization results into an MLP\n",
        "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
        "        return Y_hat"
      ],
      "metadata": {
        "id": "iqPMKNWaeF0v"
      },
      "id": "iqPMKNWaeF0v",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecomposableAttention(nn.Module):\n",
        "    def __init__(self,  embed_size, num_hiddens, num_inputs_attend=100,\n",
        "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
        "        super(DecomposableAttention, self).__init__(**kwargs)\n",
        "        #self.embedding = nn.Embedding(len(vocab), embed_size)\n",
        "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
        "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
        "        # There are 3 possible outputs: entailment, contradiction, and neutral\n",
        "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A, B = X['Premise'], X['Hypothesis']\n",
        "        beta, alpha = self.attend(A, B)\n",
        "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
        "        Y_hat = self.aggregate(V_A, V_B)\n",
        "        return Y_hat"
      ],
      "metadata": {
        "id": "kmflPFlHeGzT"
      },
      "id": "kmflPFlHeGzT",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "net = DecomposableAttention(100, 200)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    print(f\"running epoch {epoch}\")\n",
        "    for i, x in enumerate(train_loader):\n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        print(f\"running i {i}\")\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net.forward(x)\n",
        "        loss = criterion(outputs, x[\"Label\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgowrGX-0Tsv",
        "outputId": "1cc35756-5fd7-44e0-f147-2ac2634b06cd"
      },
      "id": "bgowrGX-0Tsv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running i 0\n",
            "running i 1\n",
            "running i 2\n",
            "running i 3\n",
            "running i 4\n",
            "running i 5\n",
            "running i 6\n",
            "running i 7\n",
            "running i 8\n",
            "running i 9\n",
            "running i 10\n",
            "running i 11\n",
            "running i 12\n",
            "running i 13\n",
            "running i 14\n",
            "running i 15\n",
            "running i 16\n",
            "running i 17\n",
            "running i 18\n",
            "running i 19\n",
            "running i 20\n",
            "running i 21\n",
            "running i 22\n",
            "running i 23\n",
            "running i 24\n",
            "running i 25\n",
            "running i 26\n",
            "running i 27\n",
            "running i 28\n",
            "running i 29\n",
            "running i 30\n",
            "running i 31\n",
            "running i 32\n",
            "running i 33\n",
            "running i 34\n",
            "running i 35\n",
            "running i 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**________________ END--------------------------**"
      ],
      "metadata": {
        "id": "xBYsyZYJ45G-"
      },
      "id": "xBYsyZYJ45G-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}