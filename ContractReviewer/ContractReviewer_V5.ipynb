{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12s2zq7tGkYkoNZ6R43ZmFBmHPCcGPJlz","timestamp":1669892012923}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uKWz4BPAaGz","outputId":"e4a64175-dfa2-48da-f02b-4a1b225af1f8","executionInfo":{"status":"ok","timestamp":1670245002427,"user_tz":360,"elapsed":22257,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import sys\n","dir= \"/content/drive/MyDrive//DeepLearning2022-main/ContractReviewer\"\n","os.chdir(dir)"],"metadata":{"id":"enlC7yoOAkzR","executionInfo":{"status":"ok","timestamp":1670245002427,"user_tz":360,"elapsed":4,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#!pip install bert-pytorch\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install onnx\n","!pip install fasttext\n","!pip install torchtext\n","!pip install scikit-learn\n","!pip install d2l==1.0.0-alpha1.post0\n","!pip install sklearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9KKShmrGP49","outputId":"9dfba207-bf9d-46f3-be6d-8b3ba5a0f11c","executionInfo":{"status":"ok","timestamp":1670245079718,"user_tz":360,"elapsed":77294,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 28.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 18.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 76.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 63.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx\n","  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[K     |████████████████████████████████| 13.1 MB 10.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.1.1)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from onnx) (3.19.6)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.21.6)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.12.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 7.1 MB/s \n","\u001b[?25hCollecting pybind11>=2.2\n","  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3130159 sha256=10e91366ebaee95c4fc66b2486c8be74f7449319f93765effb7216c04486f017\n","  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.10.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.12.1+cu113)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting d2l==1.0.0-alpha1.post0\n","  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 1.4 MB/s \n","\u001b[?25hCollecting jupyter\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n","Collecting matplotlib-inline\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.10.0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.7.16)\n","Collecting qtconsole\n","  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n","\u001b[K     |████████████████████████████████| 121 kB 74.9 MB/s \n","\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.0.4)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 54.3 MB/s \n","\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n","Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.15.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.19.2)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.6)\n","Collecting qtpy>=2.0.1\n","  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n","Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n","Successfully installed d2l-1.0.0a1.post0 jedi-0.18.2 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.4.0 qtpy-2.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=eea5bd6e1a0c7e9d2f47fb0c8c3cbbf6421f943a7233356c5cd38fe802f38f17\n","  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0.post1\n"]}]},{"cell_type":"code","source":["import yaml\n","import argparse\n","import time\n","import copy\n","\n","import torch\n","import torchtext\n","from torchtext import datasets\n","from torch import nn\n","from d2l import torch as d2l\n","\n","import re\n","import pandas as pd\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import classification_report"],"metadata":{"id":"xdogk3dvGVqR","executionInfo":{"status":"ok","timestamp":1670245085655,"user_tz":360,"elapsed":5941,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","train_faiss = pd.read_csv(\"data/train_faiss.csv\")\n","test_faiss = pd.read_csv(\"data/test_faiss.csv\")\n","\n","train = pd.read_csv(\"data/train.csv\")\n","test = pd.read_csv(\"data/test.csv\")\n","\n","train.rename(columns={'choice': 'label'}, inplace=True)\n","test.rename(columns={'choice': 'label'}, inplace=True)\n","train_faiss.rename(columns={'choice': 'label'}, inplace=True)\n","test_faiss.rename(columns={'choice': 'label'}, inplace=True)\n","\n","def change_label(x):\n","  torch.manual_seed(123)\n","\n","  if x == \"Entailment\":\n","    return 0\n","  elif x == \"Contradiction\":\n","    return 1\n","  else:\n","    return 2\n","\n","train[\"label\"] = train.label.apply(lambda x: change_label(x))\n","test[\"label\"]  = test.label.apply(lambda x: change_label(x))\n","\n","train = train[train['label'] != 2]\n","test = test[test['label'] != 2]\n","\n","train = pd.concat([train_faiss, train])\n","test = pd.concat([test_faiss, test])"],"metadata":{"id":"oSa6XYSQB8Me","executionInfo":{"status":"ok","timestamp":1670245113193,"user_tz":360,"elapsed":27550,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["test.label.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UzF-Sj72ooZ","executionInfo":{"status":"ok","timestamp":1670245113198,"user_tz":360,"elapsed":170,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}},"outputId":"d9b98091-32ce-4158-9ffb-94d848f18fd3"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    17728\n","0     3915\n","1      894\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["### UTILITY\n","\n","import re\n","import json\n","import torch\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from torchtext.vocab import GloVe\n","from torchtext.data import get_tokenizer\n","from nltk.tokenize import RegexpTokenizer\n","\n","\"\"\"\n","Load train, dev, test data\n","\"\"\"\n","def load():\n","\n","  global Pg\n","  Pg =  getPositionEncoding(100, d=100)\n","  Pg  = torch.FloatTensor(Pg)\n","  \"\"\"\n","  Train\n","  \"\"\"\n","  f = open('data/train.json') \n","  data_train = json.load(f)\n","\n","  kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', \n","       'nda-2', 'nda-1', 'nda-19', 'nda-12', \n","       'nda-20', 'nda-3', 'nda-18', 'nda-7', \n","       'nda-17', 'nda-8', 'nda-13', 'nda-5', \n","       'nda-4']\n","\n","  rg = len(data_train['documents'])\n","  reference_list=[]\n","  idx=0\n","  for idx in range(rg):\n","    docid = data_train['documents'][idx]['id']\n","    for ndas_key in kes:\n","      reference_list.append([docid, ndas_key,idx])\n","    idx=idx+1\n","  ref_train = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n","  f.close()\n","\n","  \"\"\"\n","  Test\n","  \"\"\"\n","  f = open('data/test.json')  \n","  data_test = json.load(f)\n","\n","  rg = len(data_test['documents'])\n","  reference_list=[]\n","  idxt=0\n","  for idxt in range(rg):\n","    #Document level\n","    docid = data_test['documents'][idxt]['id']\n","\n","    for ndas_key in kes:\n","      reference_list.append([docid, ndas_key, idxt])\n","    idxt=idxt+1\n","  ref_test = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n","\n","  \"\"\"\n","  Validation\n","  \"\"\"\n","  f = open('data/dev.json')  \n","  data_valid = json.load(f)\n","\n","  rg = len(data_valid['documents'])\n","  reference_list=[]\n","  idxt=0\n","  for idxt in range(rg):\n","    #Document level\n","    docid = data_valid['documents'][idxt]['id']\n","\n","    for ndas_key in kes:\n","      reference_list.append([docid, ndas_key, idxt])\n","    idxt=idxt+1\n","  ref_valid = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n","\n","  return data_train, ref_train, data_valid, ref_valid, data_test, ref_test\n"," \n","def load_glove(d=100):\n","\n","  global global_vectors \n","  global_vectors = GloVe(name='6B', dim=d)\n","  return global_vectors\n","\n","def get_data_non_fias(data, idx, ke, max_neutral=10):\n","  dataM=[]\n","  #Document level\n","  docid = data['documents'][idx]['id']\n","  \n","  string = data['documents'][idx]['text']\n","  file_name = data['documents'][idx]['file_name']\n","  spans = data['documents'][idx]['spans']\n","  spanall=[]\n","  for span in spans:           \n","      spanval = string[span[0]: span[1]]\n","      spanall.append(spanval)\n","  \n","  ndas = data['documents'][idx]['annotation_sets'][0]['annotations'] \n","  # Keys level\n","    \n","  hypothesis = data['labels'][ke]['hypothesis']  \n","\n","  # Key level   \n","\n","  choice =ndas[ke] ['choice']\n","  spansC = ndas[ke] ['spans']    \n","  for si in range(len(spanall)):\n","    span_nbr =si\n","    if si in spansC:\n","      val=choice\n","    else:\n","      val=\"Neutral\"\n","    premise = spanall[si]\n","    itm = [docid, file_name , ke, hypothesis, span_nbr, premise,val ]\n","    dataM.append(itm)\n","  df = pd.DataFrame(dataM ,columns = ['docid', 'file_name', 'hypotheis_key', 'hypotheis', 'span_nbr', 'premise', 'choice'   ])\n","  #\"[CLS] \" and \" [SEP] \"\" [SEP]\" \n","  df['premise'] = df['premise'].apply(preprocess)\n","  df['hypotheis']= df['hypotheis'].apply(preprocess) \n","  df['label'] = df['choice'].map(lambda s: 0 if s=='Entailment' else (1 if s== 'Contradiction' else 2 ))\n","  df['entl'] =  df['choice'].map(lambda s : 1 if s== 'Entailment' else 0)\n","  df['cont'] =  df['choice'].map(lambda s : 1 if s== 'Contradiction' else 0)\n","  df['neut'] =  df['choice'].map(lambda s : 1 if s== 'Neutral' else 0)\n","  \n","  df1=df[df.choice !='Neutral']\n","  df2=df[df.choice =='Neutral']\n","  n = min(max_neutral, len(df2)-1)\n","  df2= df2.sample(n = n)\n","  df = pd.concat([df1, df2], sort=False)\n","  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n","  #df['bert_sent']= df.apply(lambda s: [s.premise,s.hypotheis], axis=1)\n","  return df\n","\n","\n","def get_data(data, idx, ke, use_faiss=True, max_neutral=20):  \n","  \n","  #Hack for testing so that epoch's dont fail.\n","  if not use_faiss:\n","    return get_data_non_fias(data, idx, ke, max_neutral=10)\n","\n","  dataM=[]\n","  idxke= str(idx)+'|'+ke\n","  with open('samples.pickle', 'rb') as f:\n","    flt = pickle.load(f)\n","  \n","  docid = data['documents'][idx]['id']\n","  \n","  string = data['documents'][idx]['text']\n","  file_name = data['documents'][idx]['file_name']\n","  spans = data['documents'][idx]['spans']\n","  spanall=[]\n","  for span in spans:           \n","      spanval = string[span[0]: span[1]]\n","      spanall.append(spanval)\n","  \n","  ndas = data['documents'][idx]['annotation_sets'][0]['annotations'] \n","    \n","  hypothesis = data['labels'][ke]['hypothesis']  \n","\n","  choice =ndas[ke] ['choice']\n","  spansC = ndas[ke] ['spans']    \n","  for si in range(len(spanall)):\n","    span_nbr =si\n","    if si in spansC:\n","      val=choice\n","    else:\n","      val=\"Neutral\"\n","    premise = spanall[si]\n","    itm = [docid, file_name , ke, hypothesis, span_nbr, premise,val ]\n","    dataM.append(itm)\n","  df = pd.DataFrame(dataM ,columns = ['docid', 'file_name', 'hypotheis_key', 'hypotheis', 'span_nbr', 'premise', 'choice'   ])\n","  \n","  \n","  df['premise'] = df['premise'].apply(preprocess)\n","  df['hypotheis']= df['hypotheis'].apply(preprocess) \n","  df['label'] = df['choice'].map(lambda s: 0 if s=='Entailment' else (1 if s== 'Contradiction' else 2 ))\n","  df['entl'] =  df['choice'].map(lambda s : 1 if s== 'Entailment' else 0)\n","  df['cont'] =  df['choice'].map(lambda s : 1 if s== 'Contradiction' else 0)\n","  df['neut'] =  df['choice'].map(lambda s : 1 if s== 'Neutral' else 0)\n","   \n","  \n","  #df2= df2.sample(n = n)\n","  if use_faiss:\n","    df1=df[df.choice !='Neutral']\n","    spnNotNeut = df1.span_nbr.values.tolist() \n","    df2=df[df.choice =='Neutral']\n","    res = flt[idxke]\n","    res = [eval(i) for i in res]\n","    res = list(set(res)-set(spnNotNeut))\n","    res = res[:10] # Selecting top 10 from 15 items\n","      #print(res)\n","    df2 = df2[df2.span_nbr.isin(res)]\n","    df = pd.concat([df1, df2], sort=False)\n","    df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n","    return df\n","  else:\n","    df1=df[df.choice !='Neutral']\n","    df2=df[df.choice =='Neutral']\n","    n = min(max_neutral, len(df2)-1)\n","    df2= df2.sample(n = n)\n","    df = pd.concat([df1, df2], sort=False)\n","    df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']    \n","    return df\n","  \n","\n","def preprocess(sentence):\n","  sentence=str(sentence)\n","  sentence = sentence.lower().replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\").replace(\"__\",\" \").replace(\"  \",\" \")\n","  sentence=sentence.replace('{html}',\"\") \n","  cleanr = re.compile('<.*?>')\n","  cleantext = re.sub(cleanr, '', sentence)\n","  rem_url=re.sub(r'http\\S+', '',cleantext)\n","  rem_num = re.sub('[0-9]+', '', rem_url)\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  tokens = tokenizer.tokenize(rem_num)  \n","\n","  return \" \".join(tokens)\n","\n","def getPositionEncoding(max_length, d=100, n=10000):\n","    P = np.zeros((max_length, d))\n","    #print(max_length,P.shape)\n","    for k in range(max_length):      \n","      for i in np.arange(int(d/2)):\n","        denominator = np.power(n, 2*i/d)\n","        P[k, 2*i] = np.sin(k/denominator)\n","        P[k, 2*i+1] = np.cos(k/denominator)\n","    return P\n","\n","def emd(sent, max_length=100):\n","  tok = get_tokenizer(\"basic_english\")\n","  tkn = tok(sent)  \n","  ln = len(tkn)\n","  x = global_vectors.get_vecs_by_tokens(tkn)\n","  if ln<max_length:  \n","    x = torch.nn.ConstantPad2d((0, 0, 0, max_length-ln), 0)(x)\n","  else:\n","    x= x[:max_length,:]\n","  x=x+Pg \n","  return x\n"],"metadata":{"id":"s6O1DMu9xKX2","executionInfo":{"status":"ok","timestamp":1670245113704,"user_tz":360,"elapsed":617,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Will need to implement FIAS Here to update Train dataset for training"],"metadata":{"id":"4zdUF6kbAmvq"}},{"cell_type":"code","source":["#######################################################################################\n","#######################################################################################\n","import json\n","import pandas as pd\n","import pickle\n","#################\n","# Opening JSON file\n","f = open('data/train.json')  \n","# returns JSON object as \n","# a dictionary\n","data = json.load(f)\n","\n","kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n","rg = len(data['documents'])\n","reference_list=[]\n","idx=0\n","for idx in range(rg):\n","  #Document level\n","  docid = data['documents'][idx]['id']\n","  for ndas_key in kes:\n","    reference_list.append([docid, ndas_key,idx])\n","  idx=idx+1\n","ref_train = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n","f.close()\n","##############\n","# Opening JSON file\n","f = open('data/test.json')  \n","# returns JSON object as \n","# a dictionary\n","dataT = json.load(f)\n","\n","kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n","rg = len(dataT['documents'])\n","reference_list=[]\n","idxt=0\n","for idxt in range(rg):\n","  #Document level\n","  docid = dataT['documents'][idxt]['id']\n","\n","  for ndas_key in kes:\n","    reference_list.append([docid, ndas_key, idxt])\n","  idxt=idxt+1\n","ref_test = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n"," \n","# This is one time think and will take around 10 minutes\n","import os.path\n","path_to_file = \"train.pickle\"\n","if os.path.exists(path_to_file)==False:\n","    train_dataset= get_data(data, 0,'nda-3') # Dummy dataframe to create structure\n","    train_dataset = pd.DataFrame(columns= train_dataset.columns)\n","    for i in range(len(data['documents'])):\n","      for ke in kes:\n","        df2= get_data(data, i,ke)\n","        train_dataset=pd.concat([train_dataset,df2])\n","    train_dataset.to_pickle(path_to_file)\n","train = pd.read_pickle(path_to_file)\n","train = train[['hypotheis', 'premise','label']]\n","train['label'] = train['label'].astype(int)\n","\n","path_to_fileT = \"test.pickle\"\n","if os.path.exists(path_to_fileT)==False:\n","    test_dataset= get_data(dataT, 0,'nda-3') # Dummy dataframe to create structure\n","    test_dataset = pd.DataFrame(columns= test_dataset.columns)\n","    for i in range(len(dataT['documents'])):\n","      for ke in kes:\n","        df2= get_data(dataT, i,ke)\n","        test_dataset=pd.concat([test_dataset,df2])\n","    test_dataset.to_pickle(path_to_fileT)\n","test = pd.read_pickle(path_to_fileT)\n","test= test[['hypotheis', 'premise','label']]\n","test['label'] = test['label'].astype(int)\n","\n","#######################################################################################\n","#######################################################################################"],"metadata":{"id":"mIiSio8lBXlZ","executionInfo":{"status":"ok","timestamp":1670245116072,"user_tz":360,"elapsed":2381,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## End implementation of FIAS"],"metadata":{"id":"3b0M2DbzBSJV"}},{"cell_type":"code","source":["class SNLIDataset(torch.utils.data.Dataset):\n","    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n","    def __init__(self, dataset, num_steps, vocab=None):\n","        torch.manual_seed(123)\n","        self.num_steps = num_steps\n","        all_premise_tokens = d2l.tokenize(dataset[\"hypotheis\"])\n","        all_hypothesis_tokens = d2l.tokenize(dataset[\"premise\"])\n","        if vocab is None:\n","            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n","                                   min_freq=5, reserved_tokens=['<pad>'])\n","        else:\n","            self.vocab = vocab\n","        self.premises = self._pad(all_premise_tokens)\n","        self.hypotheses = self._pad(all_hypothesis_tokens)\n","        self.labels = torch.tensor(dataset[\"label\"])\n","        print('read ' + str(len(self.premises)) + ' examples')\n","\n","    def _pad(self, lines):\n","        torch.manual_seed(123)\n","        return torch.tensor([d2l.truncate_pad(\n","            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n","                         for line in lines])\n","\n","    def __getitem__(self, idx):\n","        torch.manual_seed(123)\n","        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n","\n","    def __len__(self):\n","        torch.manual_seed(123)\n","        return len(self.premises)"],"metadata":{"id":"3X8XPCpUF6_m","executionInfo":{"status":"ok","timestamp":1670245116074,"user_tz":360,"elapsed":27,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from d2l import torch as d2l"],"metadata":{"id":"JHzGwqJmMn-k","executionInfo":{"status":"ok","timestamp":1670245116076,"user_tz":360,"elapsed":25,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def mlp(num_inputs, num_hiddens, flatten):\n","    torch.manual_seed(123)\n","    net = []\n","    net.append(nn.Dropout(0.2))\n","    net.append(nn.Linear(num_inputs, num_hiddens))\n","    net.append(nn.ReLU())\n","    if flatten:\n","        net.append(nn.Flatten(start_dim=1))\n","    net.append(nn.Dropout(0.2))\n","    net.append(nn.Linear(num_hiddens, num_hiddens))\n","    net.append(nn.ReLU())\n","    if flatten:\n","        net.append(nn.Flatten(start_dim=1))\n","    return nn.Sequential(*net)"],"metadata":{"id":"c5EoyAzXMshr","executionInfo":{"status":"ok","timestamp":1670245116078,"user_tz":360,"elapsed":26,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class Attend(nn.Module):\n","    def __init__(self, num_inputs, num_hiddens, **kwargs):\n","        torch.manual_seed(123)\n","        super(Attend, self).__init__(**kwargs)\n","        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n","\n","    def forward(self, A, B):\n","        torch.manual_seed(123)\n","        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n","        # `embed_size`)\n","        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n","        # `num_hiddens`)\n","        f_A = self.f(A)\n","        f_B = self.f(B)\n","        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n","        # no. of tokens in sequence B)\n","        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n","        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n","        # `embed_size`), where sequence B is softly aligned with each token\n","        # (axis 1 of `beta`) in sequence A\n","        beta = torch.bmm(F.softmax(e, dim=-1), B)\n","        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n","        # `embed_size`), where sequence A is softly aligned with each token\n","        # (axis 1 of `alpha`) in sequence B\n","        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n","        return beta, alpha"],"metadata":{"id":"EXAl3uA9MvVU","executionInfo":{"status":"ok","timestamp":1670245116079,"user_tz":360,"elapsed":24,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Compare(nn.Module):\n","    def __init__(self, num_inputs, num_hiddens, **kwargs):\n","        torch.manual_seed(123)\n","        super(Compare, self).__init__(**kwargs)\n","        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n","\n","    def forward(self, A, B, beta, alpha):\n","        torch.manual_seed(123)\n","        V_A = self.g(torch.cat([A, beta], dim=2))\n","        V_B = self.g(torch.cat([B, alpha], dim=2))\n","        return V_A, V_B"],"metadata":{"id":"n2HVxZdlMyZ0","executionInfo":{"status":"ok","timestamp":1670245116080,"user_tz":360,"elapsed":23,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class Aggregate(nn.Module):\n","    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n","        torch.manual_seed(123)\n","        super(Aggregate, self).__init__(**kwargs)\n","        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n","        self.linear = nn.Linear(num_hiddens, num_outputs)\n","\n","    def forward(self, V_A, V_B):\n","        torch.manual_seed(123)\n","        # Sum up both sets of comparison vectors\n","        V_A = V_A.sum(dim=1)\n","        V_B = V_B.sum(dim=1)\n","        # Feed the concatenation of both summarization results into an MLP\n","        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n","        return Y_hat"],"metadata":{"id":"xV9LlZ8GM0CE","executionInfo":{"status":"ok","timestamp":1670245116333,"user_tz":360,"elapsed":35,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class DecomposableAttention(nn.Module):\n","    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n","                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n","        torch.manual_seed(123)\n","        super(DecomposableAttention, self).__init__(**kwargs)\n","        self.embedding = nn.Embedding(len(vocab), embed_size)\n","        self.attend = Attend(num_inputs_attend, num_hiddens)\n","        self.compare = Compare(num_inputs_compare, num_hiddens)\n","        # There are 3 possible outputs: entailment, contradiction, and neutral\n","        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n","\n","    def forward(self, X):\n","        torch.manual_seed(123)\n","        premises, hypotheses = X\n","        A = self.embedding(premises)\n","        B = self.embedding(hypotheses)\n","        beta, alpha = self.attend(A, B)\n","        V_A, V_B = self.compare(A, B, beta, alpha)\n","        Y_hat = self.aggregate(V_A, V_B)\n","        return Y_hat"],"metadata":{"id":"0yTdbymhM5N9","executionInfo":{"status":"ok","timestamp":1670245116336,"user_tz":360,"elapsed":35,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import numpy as np\n","\n","def reweight(cls_num_list, beta=0.9999):\n","    torch.manual_seed(123)\n","\n","    \"\"\"\n","    Implement reweighting by effective numbers\n","    :param cls_num_list: a list containing # of samples of each class\n","    :param beta: hyper-parameter for reweighting, see paper for more details\n","    :return:\n","    \"\"\"\n","    per_cls_weights = None\n","    #############################################################################\n","    # TODO: reweight each class by effective numbers                            #\n","    #############################################################################\n","    per_cls_weights = []\n","    for cls_num in cls_num_list:\n","        per_cls_weights.append((1 - beta) / (1 - (beta ** cls_num)))         \n","    #############################################################################\n","    #                              END OF YOUR CODE                             #\n","    #############################################################################\n","    return per_cls_weights\n","\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, weight=None, gamma=0.1, device='cpu'):\n","        torch.manual_seed(123)\n","        super(FocalLoss, self).__init__()\n","        assert gamma >= 0\n","        self.gamma = gamma\n","        self.weight = weight\n","        self.device = device\n","\n","    def forward(self, input, target):\n","        \"\"\"\n","        Implement forward of focal loss\n","        :param input: input predictions\n","        :param target: labels\n","        :return: tensor of focal loss in scalar\n","        \"\"\"\n","        torch.manual_seed(123)\n","        loss = None\n","        #############################################################################\n","        # TODO: Implement forward pass of the focal loss                            #\n","        #############################################################################\n","        \n","        input = input.to(self.device)\n","        target = target.to(self.device)\n","        import torch.nn.functional as F\n","        weight=torch.from_numpy(np.array(self.weight)).float().to(self.device)\n","        ce_loss = F.cross_entropy(input, target,  weight=weight)\n","        pt = torch.exp(-ce_loss) \n","        foc_loss = ((1 -pt) ** self.gamma * ce_loss)\n","        loss = foc_loss.mean()\n","        #############################################################################\n","        #                              END OF YOUR CODE                             #\n","        #############################################################################\n","        return loss"],"metadata":{"id":"6X8fcVLw2bZJ","executionInfo":{"status":"ok","timestamp":1670245116338,"user_tz":360,"elapsed":33,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters\n"],"metadata":{"id":"L3U5PFggSxJv"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","\n","def load_config_notebook():\n","  class Args:\n","    num_step = 50\n","    batch_size= 256\n","    learning_rate= 0.01\n","    reg= 0.0001\n","    epochs= 10\n","    steps= [6, 8]\n","    warmup= 0\n","    momentum= 0.75\n","    gamma=3\n","    beta= .75\n","    max_netural= 5\n","    save_best= True\n","    model= DecomposableAttention\n","    trainer= \"Adam\" #SGD can be Adam or SGD\n","    loss= \"Focal\" #CrossEntropyLoss can be Focal or CrossEntropyLoss\n","\n","  return Args()\n","\n","args = load_config_notebook()"],"metadata":{"id":"WwEfo4SAS60V","executionInfo":{"status":"ok","timestamp":1670245116341,"user_tz":360,"elapsed":31,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Training Model"],"metadata":{"id":"IBdgR-6hBgES"}},{"cell_type":"code","source":["#############################################################\n","# Create Dataset\n","#############################################################\n","torch.manual_seed(123)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","num_steps = args.num_step\n","batch_size = args.batch_size\n","num_workers = d2l.get_dataloader_workers()\n","train_set = SNLIDataset(train, num_steps)\n","test_set = SNLIDataset(test, num_steps, train_set.vocab)\n","vocab = train_set.vocab\n","train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n","                                             shuffle=False,\n","                                             num_workers=num_workers)\n","test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n","                                        shuffle=False,\n","                                        num_workers=num_workers)\n","#############################################################\n","# Set Up Model and Glove components\n","#############################################################\n","embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n","net = DecomposableAttention(vocab, embed_size, num_hiddens)\n","glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n","embeds = glove_embedding[vocab.idx_to_token]\n","net.embedding.weight.data.copy_(embeds);\n","\n","#############################################################\n","# Train Model\n","#############################################################\n","\n","torch.manual_seed(123)\n","\n","#reweight labels\n","a = train.label.value_counts()\n","cls_num_list = list([a[0],a[1],a[2]])\n","per_cls_weights =  reweight(cls_num_list, beta=args.beta)\n","\n","#hyperparam criterion\n","if args.trainer == \"Adam\":\n","  trainer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n","else:\n","  trainer = torch.optim.SGD(net.parameters(), lr=args.learning_rate,\\\n","                            momentum=args.momentum)\n","#hyperparam loss function\n","if args.loss == \"Focal\":\n","  loss = FocalLoss(weight=per_cls_weights, gamma=args.gamma).to(device)\n","else:\n","  loss = nn.CrossEntropyLoss(reduction=\"none\")\n","\n","d2l.train_ch13(net, train_iter, test_iter, loss, trainer, args.epochs, devices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"tNBfE-lU2uNm","outputId":"a738e282-4308-4661-80cf-83b7626f8d54","executionInfo":{"status":"error","timestamp":1670245121774,"user_tz":360,"elapsed":5461,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":18,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-dfca80e1d1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNLIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNLIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-1b06cf2ac3ba>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, num_steps, vocab)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_premise_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hypothesis_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpremises\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'Series'"]}]},{"cell_type":"code","source":["train.label.value_counts()"],"metadata":{"id":"eLDCApO91wl3","executionInfo":{"status":"aborted","timestamp":1670245121780,"user_tz":360,"elapsed":54,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing and Prediction"],"metadata":{"id":"F-UOOK0cB6fG"}},{"cell_type":"code","source":["def predict_snli(net, vocab, premise, hypothesis):\n","    torch.manual_seed(123)\n","    \"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\n","    net.eval()\n","    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())\n","    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\n","    label = torch.argmax(net([premise.reshape((1, -1)),\n","                           hypothesis.reshape((1, -1))]), dim=1)\n","    return 'Entailment' if label == 0 else 'Contradiction' if label == 1 \\\n","            else 'neutral'"],"metadata":{"id":"9DUJzZAvQX3-","executionInfo":{"status":"aborted","timestamp":1670245121788,"user_tz":360,"elapsed":59,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_snli(net, vocab, train.iloc[4, :][\"premise\"].split(), \\\n","             train.iloc[4, :][\"hypotheis\"].split())"],"metadata":{"id":"lVvD25ZfQzyf","executionInfo":{"status":"aborted","timestamp":1670245121790,"user_tz":360,"elapsed":60,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_hat=[]\n","for i in range(len(train)):\n","    y_hat.append(predict_snli(net, vocab, train.iloc[i, :][\"premise\"].split(), \\\n","                          train.iloc[i, :][\"hypotheis\"].split()))\n","    \n","    "],"metadata":{"id":"2UBNKcFPVpL2","executionInfo":{"status":"aborted","timestamp":1670245121792,"user_tz":360,"elapsed":61,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","\n","frequency = collections.Counter(y_hat)\n","frequency"],"metadata":{"id":"2PWi1K-tFrvm","executionInfo":{"status":"aborted","timestamp":1670245121793,"user_tz":360,"elapsed":61,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_hat_transformed = [change_label(x) for x in y_hat]"],"metadata":{"id":"55TjLyh-EcgM","executionInfo":{"status":"aborted","timestamp":1670245121795,"user_tz":360,"elapsed":62,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics"],"metadata":{"id":"WkiVus4BCc_P"}},{"cell_type":"code","source":["labels = train[\"label\"].values"],"metadata":{"id":"iowW5wwsZuZh","executionInfo":{"status":"aborted","timestamp":1670245121797,"user_tz":360,"elapsed":60,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\n {classification_report(labels, y_hat_transformed, labels=[0,1,2])}\")"],"metadata":{"id":"56fYy0-nCfeG","executionInfo":{"status":"aborted","timestamp":1670245121798,"user_tz":360,"elapsed":61,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Grid Search**"],"metadata":{"id":"xegD6oc4VIlU"}},{"cell_type":"code","source":["import pandas as pd\n","\n","res_dict = {}\n","res_dict['batch_size'] = []\n","res_dict['learning_rate'] = []\n","res_dict['reg'] = []\n","res_dict['momentum'] = []\n","res_dict['gamma'] = []\n","res_dict['beta'] = []\n","res_dict['max_neutral'] = []\n","res_dict['optimizer'] = []\n","res_dict['loss_type'] = []\n","res_dict['class_0_precision'] = []\n","res_dict['class_0_recall'] = []\n","res_dict['class_0_f1'] = []\n","res_dict['class_1_precision'] = []\n","res_dict['class_1_recall'] = []\n","res_dict['class_1_f1'] = []\n","res_dict['class_2_precision'] = []\n","res_dict['class_2_recall'] = []\n","res_dict['class_2_f1'] = []\n","res_dict['accuracy'] = []\n","\n","for batch_size in [64, 128, 256]:\n","  for learning_rate in [0.01, 0.001, 0.0001]:\n","    for reg in [0.001, 0.0001, 0.00001]:\n","      for momentum in [0.9]:\n","        for gamma in [1, 2, 3]:\n","          for beta in [0.75, 0.9, 0.99, 0.999]:\n","            for max_neutral in [5]:\n","              for optimizer in ['Adam', 'SGD']:\n","                for loss_type in ['Focal']:\n","\n","                    print(\"Experimenting with\", \\\n","                          \"batch_size:\", batch_size, \\\n","                          \", learning_rate:\", learning_rate, \\\n","                          \", momentum:\", momentum, \\\n","                          \", gamma:\", gamma, \\\n","                          \", beta:\", beta, \\\n","                          \", max_neutral:\", max_neutral, \\\n","                          \", optimizer:\", optimizer, \\\n","                          \", loss_type:\", loss_type)\n","                    #############################################################\n","                    # Create Dataset\n","                    #############################################################\n","                    torch.manual_seed(123)\n","     \n","                    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","                    num_steps = 50\n","                    batch_size = batch_size\n","                    num_workers = d2l.get_dataloader_workers()\n","                    train_set = SNLIDataset(train, num_steps)\n","                    test_set = SNLIDataset(test, num_steps, train_set.vocab)\n","                    vocab = train_set.vocab\n","                    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n","                                                             shuffle=False,\n","                                                             num_workers=num_workers)\n","                    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n","                                                             shuffle=False,\n","                                                             num_workers=num_workers)\n","               \n","                    #############################################################\n","                    # Set Up Model and Glove components\n","                    #############################################################\n","                    embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n","                    net = DecomposableAttention(vocab, embed_size, num_hiddens)\n","                    glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n","                    embeds = glove_embedding[vocab.idx_to_token]\n","                    net.embedding.weight.data.copy_(embeds);\n"," \n","                    #############################################################\n","                    # Train Model\n","                    #############################################################\n"," \n","                    torch.manual_seed(123)\n"," \n","                    #reweight labels\n","                    a = train.label.value_counts()\n","                    cls_num_list = list([a[0],a[1],a[2]])\n","                    per_cls_weights =  reweight(cls_num_list, beta=beta)\n"," \n","                    #hyperparam criterion\n","                    if optimizer == \"Adam\":\n","                       trainer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","                    else:\n","                       trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n"," \n","                    #hyperparam loss function\n","                    if loss_type == \"Focal\":\n","                      loss = FocalLoss(weight=per_cls_weights, gamma=gamma).to(device)\n","                    else:\n","                      loss = nn.CrossEntropyLoss(reduction=\"none\")\n","\n","                    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\n","\n","                    # Test Model\n","                   \n","                    y_hat=[]\n","                    for i in range(len(test)):\n","                        y_hat.append(predict_snli(net, vocab, test.iloc[i, :][\"premise\"].split(), \\\n","                                                             test.iloc[i, :][\"hypotheis\"].split()))\n","    \n","                    frequency = collections.Counter(y_hat)\n","                    y_hat_transformed = [change_label(x) for x in y_hat]\n","\n","                    labels = test[\"label\"].values\n","\n","                    df = pd.DataFrame(classification_report(labels, y_hat_transformed, labels=[0,1,2], output_dict = True))\n","                    \n","                    class_0_precision = df['0']['precision']\n","                    class_0_recall = df['0']['recall']\n","                    class_0_f1 = df['0']['f1-score']\n","                    class_1_precision = df['1']['precision']\n","                    class_1_recall = df['1']['recall']\n","                    class_1_f1 = df['1']['f1-score']\n","                    class_2_precision = df['2']['precision']\n","                    class_2_recall = df['2']['recall']\n","                    class_2_f1 = df['2']['f1-score']\n","\n","                    res_dict['batch_size'].append(batch_size)\n","                    res_dict['learning_rate'].append(learning_rate)\n","                    res_dict['reg'].append(reg)\n","                    res_dict['momentum'].append(momentum)\n","                    res_dict['gamma'].append(gamma)\n","                    res_dict['beta'].append(beta)\n","                    res_dict['max_neutral'].append(max_neutral)\n","                    res_dict['optimizer'].append(optimizer)\n","                    res_dict['loss_type'].append(loss_type)\n","\n","                    res_dict['class_0_precision'].append(class_0_precision)\n","                    res_dict['class_0_recall'].append(class_0_recall)\n","                    res_dict['class_0_f1'].append(class_0_f1)\n","\n","                    res_dict['class_1_precision'].append(class_1_precision)\n","                    res_dict['class_1_recall'].append(class_1_recall)\n","                    res_dict['class_1_f1'].append(class_1_f1)\n","\n","                    res_dict['class_2_precision'].append(class_2_precision)\n","                    res_dict['class_2_recall'].append(class_2_recall)\n","                    res_dict['class_2_f1'].append(class_2_f1)\n","\n","                    res_dict['accuracy'].append(df['accuracy'].head(1)[0])\n","\n","                    res_df = pd.DataFrame(res_dict)\n","                    res_df.to_csv('res_df.csv')\n","                    display(res_df)"],"metadata":{"id":"sXz4Haw5VAUu","executionInfo":{"status":"aborted","timestamp":1670245121799,"user_tz":360,"elapsed":61,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.shape"],"metadata":{"id":"H1be2mshhImg","executionInfo":{"status":"aborted","timestamp":1670245121801,"user_tz":360,"elapsed":63,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.shape"],"metadata":{"id":"Ge8fCxovzyLa","executionInfo":{"status":"aborted","timestamp":1670245121802,"user_tz":360,"elapsed":64,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_csv('train_faiss.csv')"],"metadata":{"id":"_w3NWSvGz1Th","executionInfo":{"status":"aborted","timestamp":1670245121805,"user_tz":360,"elapsed":67,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.to_csv('test_faiss.csv')"],"metadata":{"id":"xjfvaWjrz5Ra","executionInfo":{"status":"aborted","timestamp":1670245121806,"user_tz":360,"elapsed":67,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZarhxUYq0Hxd","executionInfo":{"status":"aborted","timestamp":1670245121808,"user_tz":360,"elapsed":67,"user":{"displayName":"Sekhar Kanuri","userId":"11698863281084522096"}}},"execution_count":null,"outputs":[]}]}