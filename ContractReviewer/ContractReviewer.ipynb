{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46j4gpvAYVR3",
        "outputId": "d8527821-8ff1-4167-c8bd-5cea48d30e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "dir= \"/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer\"\n",
        "os.chdir(dir)"
      ],
      "metadata": {
        "id": "7ccCCu-vYdY1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Load Libraries"
      ],
      "metadata": {
        "id": "og8zivPP1Vem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert-pytorch\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install onnx\n",
        "!pip install fasttext\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install d2l==1.0.0-alpha1.post0\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HssZdgvxZ6g9",
        "outputId": "25c669e8-27ed-4fee-98d7-0cf1a2ae85c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 12.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 219 kB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3159775 sha256=72d0bed7694148d985c143a2a7d0470c91274982611ea588fd3246a24a2bd002\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0-alpha1.post0\n",
            "  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.10.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.7.16)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 21.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 24.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.15.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.6)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n",
            "Successfully installed d2l-1.0.0a1.post0 jedi-0.18.2 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.4.0 qtpy-2.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=8f9561930264992bb5633148de866b3d502cec8eb2ba312db21744daf0a0fcfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import argparse\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from utility import load\n",
        "from modules import DecomposableAttention\n",
        "from data_.my_dataset import MyDataset, coll\n",
        "from losses import FocalLoss, reweight\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "TMv_zW2zZ879"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='CS7643 Contract Reviewer')\n",
        "parser.add_argument('--config', default='configs/config.yaml') \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "M3SH7aCdapK6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "DZPiywhnasyP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This method is used to load configuration\n",
        "def load_config():\n",
        "#Load configuration    \n",
        "    args = parser.parse_args()\n",
        "    with open(args.config) as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "    for key in config:\n",
        "        for k, v in config[key].items():\n",
        "            setattr(args, k, v)\n",
        "    \n",
        "    return args"
      ],
      "metadata": {
        "id": "YiwNwRDnausC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Hyperparameters"
      ],
      "metadata": {
        "id": "Si4-XTqK1sQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this when setting up configurations via notebook\n",
        "def load_config_notebook():\n",
        "  class Args:\n",
        "    batch_size= 300\n",
        "    learning_rate= 0.00001\n",
        "    reg= 0.0001\n",
        "    epochs= 5\n",
        "    steps= [6, 8]\n",
        "    warmup= 0\n",
        "    momentum= 0.9\n",
        "    gamma= 1\n",
        "    beta= .9999\n",
        "    max_netural= 10\n",
        "    save_best= True\n",
        "    model= DecomposableAttention\n",
        "\n",
        "  return Args()\n"
      ],
      "metadata": {
        "id": "km0jhXpYenbX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_test(batch_size):\n",
        "\n",
        "    data_train, ref_train, data_valid, ref_valid, data_test, ref_test = load()\n",
        "     \n",
        "    # https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders \n",
        "    \n",
        "    train_data=MyDataset(data_train, ref_train)\n",
        "    train_loader=DataLoader(train_data,batch_size=batch_size, collate_fn=coll, shuffle=True)\n",
        "\n",
        "    valid_data=MyDataset(data_valid, ref_valid)\n",
        "    valid_loader=DataLoader(valid_data,batch_size=batch_size, collate_fn=coll, shuffle=False)\n",
        "\n",
        "    test_data=MyDataset(data_test, ref_test, use_faiss=False)\n",
        "    test_loader=DataLoader(test_data,batch_size=batch_size, collate_fn=coll, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ],
      "metadata": {
        "id": "42uHSDtMawbD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_counts_training_data(train_loader):\n",
        "    entailment = 0\n",
        "    contradiction = 0\n",
        "    neutral = 0\n",
        "\n",
        "    for i, x in enumerate(train_loader):\n",
        "        a = x['Label'].bincount().cpu().numpy()\n",
        "        entailment += a[0]\n",
        "        contradiction += a[1]\n",
        "        neutral += a[2]\n",
        "\n",
        "    # We'll feed this list to the focal loss implementation.\n",
        "    cls_num_list = list([entailment,contradiction,neutral])\n",
        "    return cls_num_list\n"
      ],
      "metadata": {
        "id": "Cxg2WVazaymd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    batch_size = target.shape[0]\n",
        "\n",
        "    _, pred = torch.max(output, dim=-1)\n",
        "\n",
        "    correct = pred.eq(target).sum() * 1.0\n",
        "\n",
        "    acc = correct / batch_size\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "vF_D2Mx1a1dT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, data_loader, model, optimizer, criterion):\n",
        "    iter_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "   \n",
        "    for idx, x in enumerate(data_loader): \n",
        "        start = time.time()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model.forward(x)\n",
        "       \n",
        "        loss = criterion(outputs, x[\"Label\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()        \n",
        "        batch_acc = accuracy(outputs, x[\"Label\"])\n",
        "        losses.update(loss.item(), outputs.shape[0])\n",
        "        acc.update(batch_acc, outputs.shape[0])\n",
        "        \n",
        "        iter_time.update(time.time() - start)\n",
        "        if idx % 10 == 0:\n",
        "            print(('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                   'Time {iter_time.val:.3f} ({iter_time.avg:.3f})\\t'\n",
        "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                   'Prec @1 {top1.val:.4f} ({top1.avg:.4f})\\t')\n",
        "                  .format(epoch, idx, len(data_loader), iter_time=iter_time, loss=losses, top1=acc))"
      ],
      "metadata": {
        "id": "7xFdfiLba3eW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, val_loader, model, criterion):\n",
        "    iter_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "\n",
        "    num_class = 3\n",
        "    cm = torch.zeros(num_class, num_class)\n",
        "    # evaluation loop\n",
        "    for idx, x in enumerate(val_loader):\n",
        "        start = time.time()   \n",
        "\n",
        "        torch.no_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, x[\"Label\"])     \n",
        "        batch_acc = accuracy(out, x[\"Label\"])\n",
        "\n",
        "        # update confusion matrix\n",
        "        _, preds = torch.max(out, 1)   \n",
        "        for t, p in zip(x[\"Label\"], preds.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1       \n",
        "\n",
        "        losses.update(loss.item())\n",
        "        acc.update(batch_acc)\n",
        "\n",
        "        iter_time.update(time.time() - start)\n",
        "        if idx % 10 == 0:\n",
        "            print(('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                   'Time {iter_time.val:.3f} ({iter_time.avg:.3f})\\t')\n",
        "                  .format(epoch, idx, len(val_loader), iter_time=iter_time, loss=losses, top1=acc))\n",
        "    cm = cm / cm.sum(1)\n",
        "    per_cls_acc = cm.diag().detach().numpy().tolist()\n",
        "    \n",
        "    print(\"* Prec @1: {top1.avg:.4f}\".format(top1=acc))\n",
        "    return acc.avg, cm"
      ],
      "metadata": {
        "id": "EuBg9VDwa5gS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    epoch += 1\n",
        "    if epoch <= args.warmup:\n",
        "        lr = args.learning_rate * epoch / args.warmup\n",
        "    elif epoch > args.steps[1]:\n",
        "        lr = args.learning_rate * 0.01\n",
        "    elif epoch > args.steps[0]:\n",
        "        lr = args.learning_rate * 0.1\n",
        "    else:\n",
        "        lr = args.learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "m9OAZcHka7Vb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and save model"
      ],
      "metadata": {
        "id": "h-r_SCzna1i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  #Load config for command line\n",
        "  #args = load_config()\n",
        "  print(\"\\r Loading config\")\n",
        "  #Load config for jupyter notebook\n",
        "  args = load_config_notebook()\n",
        "\n",
        "  # Load Data\n",
        "  print(\"\\r Loading data\")\n",
        "  train_loader, _ , test_loader = load_train_test(args.batch_size)\n",
        "\n",
        "  #Reweight training    \n",
        "  print(\"\\r reweighting data\")\n",
        "  #cls_num_list= list(get_counts_training_data(train_loader))\n",
        "  cls_num_list= list([0.00020353383324465444, 0.0006849963955165558, 0.00010011808071699749])\n",
        "  per_cls_weights = reweight(cls_num_list, beta=args.beta)\n",
        "\n",
        "  print(\"\\r Setting up Module\")\n",
        "  #https://github.gatech.edu/Sgudiduri3/DeepLearning2022/blob/main/dataM_focal_loss_20221121.ipynb\n",
        "  net = DecomposableAttention(100, 200).to(device)\n",
        "\n",
        "  #Optimization using focal loss\n",
        "  criterion = FocalLoss(weight=per_cls_weights, gamma=args.gamma).to(device)\n",
        "  optimizer = optim.SGD(net.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
        "\n",
        "  # Loop through epoch\n",
        "  # Loop through dataset\n",
        "  print(\"\\r Training and Testing\")\n",
        "  best = 0.0\n",
        "  best_cm = None\n",
        "  best_model = None\n",
        "  for epoch in range(args.epochs):  # loop over the dataset multiple times \\\n",
        "      adjust_learning_rate(optimizer, epoch, args)\n",
        "\n",
        "      # train loop\n",
        "      train(epoch, train_loader, net, optimizer, criterion)\n",
        "\n",
        "      # validation loop\n",
        "      acc, cm = validate(epoch, test_loader, net, criterion)\n",
        "\n",
        "      if acc > best:\n",
        "          best = acc\n",
        "          best_cm = cm\n",
        "          best_model = copy.deepcopy(net)\n",
        "    \n",
        "  print('\\r Best Prec @1 Acccuracy: {:.4f}'.format(best))\n",
        "   \n",
        "  if args.save_best:\n",
        "      torch.save(best_model.state_dict(), './checkpoints/' + 'decomposable_attention.pth')    \n",
        "  print('\\r Finished Training')"
      ],
      "metadata": {
        "id": "5vg9RwKsa9CJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogOuP07flxiF",
        "outputId": "b77b6642-8e5b-455d-b031-c1606e69b5cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r Loading config\n",
            " Loading data\n",
            " reweighting data\n",
            " Setting up Module\n",
            " Training and Testing\n",
            "Epoch: [0][0/360]\tTime 2.590 (2.590)\tLoss 2.2742 (2.2742)\tPrec @1 0.0317 (0.0317)\t\n",
            "Epoch: [0][10/360]\tTime 0.017 (0.251)\tLoss 0.1873 (0.3579)\tPrec @1 0.9050 (0.8199)\t\n",
            "Epoch: [0][20/360]\tTime 0.017 (0.139)\tLoss 0.0506 (0.2396)\tPrec @1 0.8929 (0.8575)\t\n",
            "Epoch: [0][30/360]\tTime 0.017 (0.100)\tLoss 0.0590 (0.1830)\tPrec @1 0.8889 (0.8681)\t\n",
            "Epoch: [0][40/360]\tTime 0.017 (0.080)\tLoss 0.0604 (0.1505)\tPrec @1 0.8850 (0.8750)\t\n",
            "Epoch: [0][50/360]\tTime 0.017 (0.067)\tLoss 0.0467 (0.1316)\tPrec @1 0.9009 (0.8776)\t\n",
            "Epoch: [0][60/360]\tTime 0.017 (0.059)\tLoss 0.0416 (0.1180)\tPrec @1 0.9050 (0.8800)\t\n",
            "Epoch: [0][70/360]\tTime 0.016 (0.053)\tLoss 0.0461 (0.1069)\tPrec @1 0.8929 (0.8835)\t\n",
            "Epoch: [0][80/360]\tTime 0.016 (0.049)\tLoss 0.0238 (0.0980)\tPrec @1 0.9302 (0.8871)\t\n",
            "Epoch: [0][90/360]\tTime 0.016 (0.045)\tLoss 0.0285 (0.0916)\tPrec @1 0.9174 (0.8886)\t\n",
            "Epoch: [0][100/360]\tTime 0.017 (0.042)\tLoss 0.0507 (0.0868)\tPrec @1 0.8658 (0.8888)\t\n",
            "Epoch: [0][110/360]\tTime 0.016 (0.040)\tLoss 0.0319 (0.0825)\tPrec @1 0.9132 (0.8896)\t\n",
            "Epoch: [0][120/360]\tTime 0.017 (0.038)\tLoss 0.0407 (0.0791)\tPrec @1 0.8969 (0.8900)\t\n",
            "Epoch: [0][130/360]\tTime 0.023 (0.037)\tLoss 0.0885 (0.0775)\tPrec @1 0.8333 (0.8888)\t\n",
            "Epoch: [0][140/360]\tTime 0.017 (0.035)\tLoss 0.0158 (0.0747)\tPrec @1 0.9569 (0.8897)\t\n",
            "Epoch: [0][150/360]\tTime 0.016 (0.034)\tLoss 0.0377 (0.0728)\tPrec @1 0.8969 (0.8894)\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer/utility.py:195: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][160/360]\tTime 0.017 (0.033)\tLoss 0.0340 (0.0708)\tPrec @1 0.9050 (0.8898)\t\n",
            "Epoch: [0][170/360]\tTime 0.016 (0.032)\tLoss 0.0406 (0.0691)\tPrec @1 0.8889 (0.8900)\t\n",
            "Epoch: [0][180/360]\tTime 0.017 (0.031)\tLoss 0.0597 (0.0674)\tPrec @1 0.8734 (0.8904)\t\n",
            "Epoch: [0][190/360]\tTime 0.017 (0.030)\tLoss 0.0473 (0.0662)\tPrec @1 0.8889 (0.8905)\t\n",
            "Epoch: [0][200/360]\tTime 0.016 (0.030)\tLoss 0.0352 (0.0649)\tPrec @1 0.9091 (0.8905)\t\n",
            "Epoch: [0][210/360]\tTime 0.018 (0.029)\tLoss 0.0680 (0.0639)\tPrec @1 0.8584 (0.8906)\t\n",
            "Epoch: [0][220/360]\tTime 0.017 (0.029)\tLoss 0.0302 (0.0630)\tPrec @1 0.9009 (0.8907)\t\n",
            "Epoch: [0][230/360]\tTime 0.017 (0.028)\tLoss 0.0395 (0.0619)\tPrec @1 0.9009 (0.8910)\t\n",
            "Epoch: [0][240/360]\tTime 0.017 (0.028)\tLoss 0.0429 (0.0609)\tPrec @1 0.8889 (0.8915)\t\n",
            "Epoch: [0][250/360]\tTime 0.017 (0.027)\tLoss 0.0356 (0.0600)\tPrec @1 0.9009 (0.8916)\t\n",
            "Epoch: [0][260/360]\tTime 0.017 (0.027)\tLoss 0.0354 (0.0590)\tPrec @1 0.9050 (0.8922)\t\n",
            "Epoch: [0][270/360]\tTime 0.017 (0.026)\tLoss 0.0430 (0.0582)\tPrec @1 0.8811 (0.8924)\t\n",
            "Epoch: [0][280/360]\tTime 0.017 (0.026)\tLoss 0.0572 (0.0575)\tPrec @1 0.8734 (0.8928)\t\n",
            "Epoch: [0][290/360]\tTime 0.016 (0.026)\tLoss 0.0349 (0.0568)\tPrec @1 0.9091 (0.8931)\t\n",
            "Epoch: [0][300/360]\tTime 0.017 (0.025)\tLoss 0.0241 (0.0561)\tPrec @1 0.9217 (0.8935)\t\n",
            "Epoch: [0][310/360]\tTime 0.017 (0.025)\tLoss 0.0455 (0.0553)\tPrec @1 0.8850 (0.8940)\t\n",
            "Epoch: [0][320/360]\tTime 0.016 (0.025)\tLoss 0.0190 (0.0546)\tPrec @1 0.9259 (0.8943)\t\n",
            "Epoch: [0][330/360]\tTime 0.018 (0.025)\tLoss 0.0515 (0.0542)\tPrec @1 0.8696 (0.8942)\t\n",
            "Epoch: [0][340/360]\tTime 0.018 (0.024)\tLoss 0.0244 (0.0537)\tPrec @1 0.9174 (0.8942)\t\n",
            "Epoch: [0][350/360]\tTime 0.017 (0.024)\tLoss 0.0429 (0.0536)\tPrec @1 0.8811 (0.8937)\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer/utility.py:131: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/105]\tTime 0.026 (0.026)\t\n",
            "Epoch: [0][10/105]\tTime 0.034 (0.028)\t\n",
            "Epoch: [0][20/105]\tTime 0.027 (0.028)\t\n",
            "Epoch: [0][30/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [0][40/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [0][50/105]\tTime 0.028 (0.027)\t\n",
            "Epoch: [0][60/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [0][70/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [0][80/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [0][90/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [0][100/105]\tTime 0.028 (0.027)\t\n",
            "* Prec @1: 0.8927\n",
            "Epoch: [1][0/360]\tTime 0.017 (0.017)\tLoss 0.0354 (0.0354)\tPrec @1 0.9050 (0.9050)\t\n",
            "Epoch: [1][10/360]\tTime 0.017 (0.017)\tLoss 0.0337 (0.0349)\tPrec @1 0.9050 (0.9005)\t\n",
            "Epoch: [1][20/360]\tTime 0.018 (0.017)\tLoss 0.0450 (0.0367)\tPrec @1 0.8929 (0.8997)\t\n",
            "Epoch: [1][30/360]\tTime 0.017 (0.017)\tLoss 0.0459 (0.0397)\tPrec @1 0.8889 (0.8967)\t\n",
            "Epoch: [1][40/360]\tTime 0.017 (0.017)\tLoss 0.0480 (0.0400)\tPrec @1 0.8850 (0.8967)\t\n",
            "Epoch: [1][50/360]\tTime 0.017 (0.017)\tLoss 0.0411 (0.0411)\tPrec @1 0.9009 (0.8950)\t\n",
            "Epoch: [1][60/360]\tTime 0.017 (0.017)\tLoss 0.0318 (0.0410)\tPrec @1 0.9050 (0.8946)\t\n",
            "Epoch: [1][70/360]\tTime 0.017 (0.017)\tLoss 0.0434 (0.0398)\tPrec @1 0.8929 (0.8960)\t\n",
            "Epoch: [1][80/360]\tTime 0.016 (0.017)\tLoss 0.0184 (0.0385)\tPrec @1 0.9302 (0.8981)\t\n",
            "Epoch: [1][90/360]\tTime 0.017 (0.017)\tLoss 0.0312 (0.0383)\tPrec @1 0.9174 (0.8984)\t\n",
            "Epoch: [1][100/360]\tTime 0.017 (0.017)\tLoss 0.0410 (0.0385)\tPrec @1 0.8658 (0.8976)\t\n",
            "Epoch: [1][110/360]\tTime 0.016 (0.017)\tLoss 0.0310 (0.0385)\tPrec @1 0.9132 (0.8976)\t\n",
            "Epoch: [1][120/360]\tTime 0.018 (0.017)\tLoss 0.0413 (0.0387)\tPrec @1 0.8969 (0.8974)\t\n",
            "Epoch: [1][130/360]\tTime 0.018 (0.017)\tLoss 0.0878 (0.0401)\tPrec @1 0.8333 (0.8956)\t\n",
            "Epoch: [1][140/360]\tTime 0.016 (0.017)\tLoss 0.0139 (0.0398)\tPrec @1 0.9569 (0.8960)\t\n",
            "Epoch: [1][150/360]\tTime 0.017 (0.017)\tLoss 0.0369 (0.0402)\tPrec @1 0.8969 (0.8953)\t\n",
            "Epoch: [1][160/360]\tTime 0.017 (0.017)\tLoss 0.0324 (0.0401)\tPrec @1 0.9050 (0.8953)\t\n",
            "Epoch: [1][170/360]\tTime 0.017 (0.017)\tLoss 0.0375 (0.0401)\tPrec @1 0.8889 (0.8952)\t\n",
            "Epoch: [1][180/360]\tTime 0.017 (0.017)\tLoss 0.0621 (0.0399)\tPrec @1 0.8734 (0.8953)\t\n",
            "Epoch: [1][190/360]\tTime 0.016 (0.017)\tLoss 0.0440 (0.0401)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [1][200/360]\tTime 0.017 (0.017)\tLoss 0.0352 (0.0401)\tPrec @1 0.9091 (0.8949)\t\n",
            "Epoch: [1][210/360]\tTime 0.018 (0.017)\tLoss 0.0724 (0.0403)\tPrec @1 0.8584 (0.8948)\t\n",
            "Epoch: [1][220/360]\tTime 0.017 (0.017)\tLoss 0.0287 (0.0403)\tPrec @1 0.9009 (0.8947)\t\n",
            "Epoch: [1][230/360]\tTime 0.017 (0.017)\tLoss 0.0352 (0.0403)\tPrec @1 0.9009 (0.8948)\t\n",
            "Epoch: [1][240/360]\tTime 0.018 (0.017)\tLoss 0.0426 (0.0400)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [1][250/360]\tTime 0.019 (0.017)\tLoss 0.0353 (0.0400)\tPrec @1 0.9009 (0.8951)\t\n",
            "Epoch: [1][260/360]\tTime 0.017 (0.017)\tLoss 0.0357 (0.0398)\tPrec @1 0.9050 (0.8956)\t\n",
            "Epoch: [1][270/360]\tTime 0.017 (0.017)\tLoss 0.0408 (0.0397)\tPrec @1 0.8811 (0.8956)\t\n",
            "Epoch: [1][280/360]\tTime 0.017 (0.017)\tLoss 0.0599 (0.0396)\tPrec @1 0.8734 (0.8959)\t\n",
            "Epoch: [1][290/360]\tTime 0.017 (0.017)\tLoss 0.0327 (0.0395)\tPrec @1 0.9091 (0.8961)\t\n",
            "Epoch: [1][300/360]\tTime 0.016 (0.017)\tLoss 0.0248 (0.0393)\tPrec @1 0.9217 (0.8964)\t\n",
            "Epoch: [1][310/360]\tTime 0.016 (0.017)\tLoss 0.0477 (0.0390)\tPrec @1 0.8850 (0.8968)\t\n",
            "Epoch: [1][320/360]\tTime 0.016 (0.017)\tLoss 0.0182 (0.0388)\tPrec @1 0.9259 (0.8971)\t\n",
            "Epoch: [1][330/360]\tTime 0.017 (0.017)\tLoss 0.0535 (0.0389)\tPrec @1 0.8696 (0.8969)\t\n",
            "Epoch: [1][340/360]\tTime 0.017 (0.017)\tLoss 0.0230 (0.0389)\tPrec @1 0.9174 (0.8968)\t\n",
            "Epoch: [1][350/360]\tTime 0.017 (0.017)\tLoss 0.0429 (0.0392)\tPrec @1 0.8811 (0.8962)\t\n",
            "Epoch: [1][0/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][10/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][20/105]\tTime 0.030 (0.027)\t\n",
            "Epoch: [1][30/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][40/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [1][50/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][60/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][70/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [1][80/105]\tTime 0.032 (0.027)\t\n",
            "Epoch: [1][90/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [1][100/105]\tTime 0.029 (0.027)\t\n",
            "* Prec @1: 0.8923\n",
            "Epoch: [2][0/360]\tTime 0.017 (0.017)\tLoss 0.0375 (0.0375)\tPrec @1 0.9050 (0.9050)\t\n",
            "Epoch: [2][10/360]\tTime 0.016 (0.017)\tLoss 0.0324 (0.0359)\tPrec @1 0.9050 (0.9005)\t\n",
            "Epoch: [2][20/360]\tTime 0.017 (0.017)\tLoss 0.0472 (0.0371)\tPrec @1 0.8929 (0.8997)\t\n",
            "Epoch: [2][30/360]\tTime 0.017 (0.017)\tLoss 0.0430 (0.0400)\tPrec @1 0.8889 (0.8967)\t\n",
            "Epoch: [2][40/360]\tTime 0.016 (0.017)\tLoss 0.0484 (0.0401)\tPrec @1 0.8850 (0.8967)\t\n",
            "Epoch: [2][50/360]\tTime 0.016 (0.017)\tLoss 0.0388 (0.0411)\tPrec @1 0.9009 (0.8950)\t\n",
            "Epoch: [2][60/360]\tTime 0.017 (0.017)\tLoss 0.0311 (0.0408)\tPrec @1 0.9050 (0.8946)\t\n",
            "Epoch: [2][70/360]\tTime 0.016 (0.017)\tLoss 0.0414 (0.0398)\tPrec @1 0.8929 (0.8960)\t\n",
            "Epoch: [2][80/360]\tTime 0.016 (0.017)\tLoss 0.0182 (0.0385)\tPrec @1 0.9302 (0.8981)\t\n",
            "Epoch: [2][90/360]\tTime 0.017 (0.017)\tLoss 0.0291 (0.0383)\tPrec @1 0.9174 (0.8984)\t\n",
            "Epoch: [2][100/360]\tTime 0.017 (0.017)\tLoss 0.0414 (0.0385)\tPrec @1 0.8658 (0.8976)\t\n",
            "Epoch: [2][110/360]\tTime 0.016 (0.017)\tLoss 0.0298 (0.0384)\tPrec @1 0.9132 (0.8976)\t\n",
            "Epoch: [2][120/360]\tTime 0.017 (0.017)\tLoss 0.0397 (0.0386)\tPrec @1 0.8969 (0.8974)\t\n",
            "Epoch: [2][130/360]\tTime 0.018 (0.017)\tLoss 0.0849 (0.0400)\tPrec @1 0.8333 (0.8956)\t\n",
            "Epoch: [2][140/360]\tTime 0.016 (0.017)\tLoss 0.0137 (0.0397)\tPrec @1 0.9569 (0.8960)\t\n",
            "Epoch: [2][150/360]\tTime 0.017 (0.017)\tLoss 0.0362 (0.0400)\tPrec @1 0.8969 (0.8953)\t\n",
            "Epoch: [2][160/360]\tTime 0.017 (0.017)\tLoss 0.0319 (0.0399)\tPrec @1 0.9050 (0.8953)\t\n",
            "Epoch: [2][170/360]\tTime 0.016 (0.017)\tLoss 0.0373 (0.0399)\tPrec @1 0.8889 (0.8952)\t\n",
            "Epoch: [2][180/360]\tTime 0.017 (0.017)\tLoss 0.0571 (0.0398)\tPrec @1 0.8734 (0.8953)\t\n",
            "Epoch: [2][190/360]\tTime 0.018 (0.017)\tLoss 0.0431 (0.0399)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [2][200/360]\tTime 0.016 (0.017)\tLoss 0.0339 (0.0399)\tPrec @1 0.9091 (0.8949)\t\n",
            "Epoch: [2][210/360]\tTime 0.017 (0.017)\tLoss 0.0718 (0.0400)\tPrec @1 0.8584 (0.8948)\t\n",
            "Epoch: [2][220/360]\tTime 0.017 (0.017)\tLoss 0.0283 (0.0401)\tPrec @1 0.9009 (0.8947)\t\n",
            "Epoch: [2][230/360]\tTime 0.017 (0.017)\tLoss 0.0366 (0.0401)\tPrec @1 0.9009 (0.8948)\t\n",
            "Epoch: [2][240/360]\tTime 0.018 (0.017)\tLoss 0.0433 (0.0399)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [2][250/360]\tTime 0.017 (0.017)\tLoss 0.0338 (0.0399)\tPrec @1 0.9009 (0.8951)\t\n",
            "Epoch: [2][260/360]\tTime 0.016 (0.017)\tLoss 0.0355 (0.0396)\tPrec @1 0.9050 (0.8956)\t\n",
            "Epoch: [2][270/360]\tTime 0.017 (0.017)\tLoss 0.0476 (0.0395)\tPrec @1 0.8811 (0.8956)\t\n",
            "Epoch: [2][280/360]\tTime 0.017 (0.017)\tLoss 0.0565 (0.0394)\tPrec @1 0.8734 (0.8959)\t\n",
            "Epoch: [2][290/360]\tTime 0.017 (0.017)\tLoss 0.0330 (0.0393)\tPrec @1 0.9091 (0.8961)\t\n",
            "Epoch: [2][300/360]\tTime 0.016 (0.017)\tLoss 0.0242 (0.0391)\tPrec @1 0.9217 (0.8964)\t\n",
            "Epoch: [2][310/360]\tTime 0.017 (0.017)\tLoss 0.0472 (0.0388)\tPrec @1 0.8850 (0.8968)\t\n",
            "Epoch: [2][320/360]\tTime 0.017 (0.017)\tLoss 0.0179 (0.0386)\tPrec @1 0.9259 (0.8971)\t\n",
            "Epoch: [2][330/360]\tTime 0.017 (0.017)\tLoss 0.0521 (0.0387)\tPrec @1 0.8696 (0.8969)\t\n",
            "Epoch: [2][340/360]\tTime 0.016 (0.017)\tLoss 0.0244 (0.0386)\tPrec @1 0.9174 (0.8968)\t\n",
            "Epoch: [2][350/360]\tTime 0.016 (0.017)\tLoss 0.0411 (0.0389)\tPrec @1 0.8811 (0.8962)\t\n",
            "Epoch: [2][0/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [2][10/105]\tTime 0.029 (0.029)\t\n",
            "Epoch: [2][20/105]\tTime 0.026 (0.030)\t\n",
            "Epoch: [2][30/105]\tTime 0.026 (0.029)\t\n",
            "Epoch: [2][40/105]\tTime 0.026 (0.028)\t\n",
            "Epoch: [2][50/105]\tTime 0.027 (0.028)\t\n",
            "Epoch: [2][60/105]\tTime 0.027 (0.028)\t\n",
            "Epoch: [2][70/105]\tTime 0.027 (0.028)\t\n",
            "Epoch: [2][80/105]\tTime 0.026 (0.028)\t\n",
            "Epoch: [2][90/105]\tTime 0.025 (0.027)\t\n",
            "Epoch: [2][100/105]\tTime 0.028 (0.027)\t\n",
            "* Prec @1: 0.8925\n",
            "Epoch: [3][0/360]\tTime 0.016 (0.016)\tLoss 0.0366 (0.0366)\tPrec @1 0.9050 (0.9050)\t\n",
            "Epoch: [3][10/360]\tTime 0.016 (0.017)\tLoss 0.0340 (0.0351)\tPrec @1 0.9050 (0.9005)\t\n",
            "Epoch: [3][20/360]\tTime 0.017 (0.017)\tLoss 0.0438 (0.0363)\tPrec @1 0.8929 (0.8997)\t\n",
            "Epoch: [3][30/360]\tTime 0.016 (0.017)\tLoss 0.0422 (0.0395)\tPrec @1 0.8889 (0.8967)\t\n",
            "Epoch: [3][40/360]\tTime 0.017 (0.017)\tLoss 0.0475 (0.0395)\tPrec @1 0.8850 (0.8967)\t\n",
            "Epoch: [3][50/360]\tTime 0.016 (0.017)\tLoss 0.0374 (0.0404)\tPrec @1 0.9009 (0.8950)\t\n",
            "Epoch: [3][60/360]\tTime 0.017 (0.017)\tLoss 0.0311 (0.0402)\tPrec @1 0.9050 (0.8946)\t\n",
            "Epoch: [3][70/360]\tTime 0.016 (0.017)\tLoss 0.0399 (0.0392)\tPrec @1 0.8929 (0.8960)\t\n",
            "Epoch: [3][80/360]\tTime 0.016 (0.017)\tLoss 0.0198 (0.0379)\tPrec @1 0.9302 (0.8981)\t\n",
            "Epoch: [3][90/360]\tTime 0.016 (0.017)\tLoss 0.0271 (0.0378)\tPrec @1 0.9174 (0.8984)\t\n",
            "Epoch: [3][100/360]\tTime 0.017 (0.017)\tLoss 0.0408 (0.0380)\tPrec @1 0.8658 (0.8976)\t\n",
            "Epoch: [3][110/360]\tTime 0.017 (0.017)\tLoss 0.0312 (0.0379)\tPrec @1 0.9132 (0.8976)\t\n",
            "Epoch: [3][120/360]\tTime 0.016 (0.017)\tLoss 0.0412 (0.0382)\tPrec @1 0.8969 (0.8974)\t\n",
            "Epoch: [3][130/360]\tTime 0.018 (0.017)\tLoss 0.0800 (0.0395)\tPrec @1 0.8333 (0.8956)\t\n",
            "Epoch: [3][140/360]\tTime 0.016 (0.017)\tLoss 0.0136 (0.0393)\tPrec @1 0.9569 (0.8960)\t\n",
            "Epoch: [3][150/360]\tTime 0.017 (0.017)\tLoss 0.0376 (0.0396)\tPrec @1 0.8969 (0.8953)\t\n",
            "Epoch: [3][160/360]\tTime 0.017 (0.017)\tLoss 0.0322 (0.0395)\tPrec @1 0.9050 (0.8953)\t\n",
            "Epoch: [3][170/360]\tTime 0.016 (0.017)\tLoss 0.0368 (0.0395)\tPrec @1 0.8889 (0.8952)\t\n",
            "Epoch: [3][180/360]\tTime 0.017 (0.017)\tLoss 0.0585 (0.0393)\tPrec @1 0.8734 (0.8953)\t\n",
            "Epoch: [3][190/360]\tTime 0.017 (0.017)\tLoss 0.0443 (0.0394)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [3][200/360]\tTime 0.017 (0.017)\tLoss 0.0341 (0.0395)\tPrec @1 0.9091 (0.8949)\t\n",
            "Epoch: [3][210/360]\tTime 0.019 (0.017)\tLoss 0.0695 (0.0397)\tPrec @1 0.8584 (0.8948)\t\n",
            "Epoch: [3][220/360]\tTime 0.016 (0.017)\tLoss 0.0287 (0.0397)\tPrec @1 0.9009 (0.8947)\t\n",
            "Epoch: [3][230/360]\tTime 0.017 (0.017)\tLoss 0.0353 (0.0397)\tPrec @1 0.9009 (0.8948)\t\n",
            "Epoch: [3][240/360]\tTime 0.017 (0.017)\tLoss 0.0430 (0.0395)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [3][250/360]\tTime 0.017 (0.017)\tLoss 0.0338 (0.0395)\tPrec @1 0.9009 (0.8951)\t\n",
            "Epoch: [3][260/360]\tTime 0.017 (0.017)\tLoss 0.0356 (0.0392)\tPrec @1 0.9050 (0.8956)\t\n",
            "Epoch: [3][270/360]\tTime 0.017 (0.017)\tLoss 0.0443 (0.0391)\tPrec @1 0.8811 (0.8956)\t\n",
            "Epoch: [3][280/360]\tTime 0.017 (0.017)\tLoss 0.0585 (0.0390)\tPrec @1 0.8734 (0.8959)\t\n",
            "Epoch: [3][290/360]\tTime 0.016 (0.017)\tLoss 0.0328 (0.0389)\tPrec @1 0.9091 (0.8961)\t\n",
            "Epoch: [3][300/360]\tTime 0.018 (0.017)\tLoss 0.0240 (0.0387)\tPrec @1 0.9217 (0.8964)\t\n",
            "Epoch: [3][310/360]\tTime 0.017 (0.017)\tLoss 0.0461 (0.0385)\tPrec @1 0.8850 (0.8968)\t\n",
            "Epoch: [3][320/360]\tTime 0.017 (0.017)\tLoss 0.0189 (0.0383)\tPrec @1 0.9259 (0.8971)\t\n",
            "Epoch: [3][330/360]\tTime 0.017 (0.017)\tLoss 0.0551 (0.0384)\tPrec @1 0.8696 (0.8969)\t\n",
            "Epoch: [3][340/360]\tTime 0.018 (0.017)\tLoss 0.0219 (0.0384)\tPrec @1 0.9174 (0.8968)\t\n",
            "Epoch: [3][350/360]\tTime 0.017 (0.017)\tLoss 0.0412 (0.0387)\tPrec @1 0.8811 (0.8962)\t\n",
            "Epoch: [3][0/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [3][10/105]\tTime 0.028 (0.027)\t\n",
            "Epoch: [3][20/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [3][30/105]\tTime 0.029 (0.027)\t\n",
            "Epoch: [3][40/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [3][50/105]\tTime 0.028 (0.027)\t\n",
            "Epoch: [3][60/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [3][70/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [3][80/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [3][90/105]\tTime 0.031 (0.027)\t\n",
            "Epoch: [3][100/105]\tTime 0.027 (0.027)\t\n",
            "* Prec @1: 0.8926\n",
            "Epoch: [4][0/360]\tTime 0.017 (0.017)\tLoss 0.0372 (0.0372)\tPrec @1 0.9050 (0.9050)\t\n",
            "Epoch: [4][10/360]\tTime 0.016 (0.017)\tLoss 0.0325 (0.0345)\tPrec @1 0.9050 (0.9005)\t\n",
            "Epoch: [4][20/360]\tTime 0.017 (0.017)\tLoss 0.0442 (0.0360)\tPrec @1 0.8929 (0.8997)\t\n",
            "Epoch: [4][30/360]\tTime 0.017 (0.017)\tLoss 0.0434 (0.0393)\tPrec @1 0.8889 (0.8967)\t\n",
            "Epoch: [4][40/360]\tTime 0.017 (0.017)\tLoss 0.0453 (0.0393)\tPrec @1 0.8850 (0.8967)\t\n",
            "Epoch: [4][50/360]\tTime 0.017 (0.017)\tLoss 0.0382 (0.0403)\tPrec @1 0.9009 (0.8950)\t\n",
            "Epoch: [4][60/360]\tTime 0.016 (0.017)\tLoss 0.0320 (0.0401)\tPrec @1 0.9050 (0.8946)\t\n",
            "Epoch: [4][70/360]\tTime 0.017 (0.017)\tLoss 0.0398 (0.0391)\tPrec @1 0.8929 (0.8960)\t\n",
            "Epoch: [4][80/360]\tTime 0.017 (0.017)\tLoss 0.0172 (0.0378)\tPrec @1 0.9302 (0.8981)\t\n",
            "Epoch: [4][90/360]\tTime 0.016 (0.017)\tLoss 0.0313 (0.0377)\tPrec @1 0.9174 (0.8984)\t\n",
            "Epoch: [4][100/360]\tTime 0.017 (0.017)\tLoss 0.0421 (0.0379)\tPrec @1 0.8658 (0.8976)\t\n",
            "Epoch: [4][110/360]\tTime 0.016 (0.017)\tLoss 0.0277 (0.0378)\tPrec @1 0.9132 (0.8976)\t\n",
            "Epoch: [4][120/360]\tTime 0.016 (0.017)\tLoss 0.0414 (0.0380)\tPrec @1 0.8969 (0.8974)\t\n",
            "Epoch: [4][130/360]\tTime 0.018 (0.017)\tLoss 0.0776 (0.0393)\tPrec @1 0.8333 (0.8956)\t\n",
            "Epoch: [4][140/360]\tTime 0.016 (0.017)\tLoss 0.0135 (0.0391)\tPrec @1 0.9569 (0.8960)\t\n",
            "Epoch: [4][150/360]\tTime 0.018 (0.017)\tLoss 0.0353 (0.0394)\tPrec @1 0.8969 (0.8953)\t\n",
            "Epoch: [4][160/360]\tTime 0.016 (0.017)\tLoss 0.0322 (0.0393)\tPrec @1 0.9050 (0.8953)\t\n",
            "Epoch: [4][170/360]\tTime 0.017 (0.017)\tLoss 0.0362 (0.0393)\tPrec @1 0.8889 (0.8952)\t\n",
            "Epoch: [4][180/360]\tTime 0.018 (0.017)\tLoss 0.0599 (0.0392)\tPrec @1 0.8734 (0.8953)\t\n",
            "Epoch: [4][190/360]\tTime 0.017 (0.017)\tLoss 0.0457 (0.0394)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [4][200/360]\tTime 0.017 (0.017)\tLoss 0.0330 (0.0394)\tPrec @1 0.9091 (0.8949)\t\n",
            "Epoch: [4][210/360]\tTime 0.017 (0.017)\tLoss 0.0677 (0.0395)\tPrec @1 0.8584 (0.8948)\t\n",
            "Epoch: [4][220/360]\tTime 0.017 (0.017)\tLoss 0.0279 (0.0396)\tPrec @1 0.9009 (0.8947)\t\n",
            "Epoch: [4][230/360]\tTime 0.017 (0.017)\tLoss 0.0370 (0.0395)\tPrec @1 0.9009 (0.8948)\t\n",
            "Epoch: [4][240/360]\tTime 0.016 (0.017)\tLoss 0.0453 (0.0393)\tPrec @1 0.8889 (0.8951)\t\n",
            "Epoch: [4][250/360]\tTime 0.017 (0.017)\tLoss 0.0333 (0.0393)\tPrec @1 0.9009 (0.8951)\t\n",
            "Epoch: [4][260/360]\tTime 0.016 (0.017)\tLoss 0.0356 (0.0391)\tPrec @1 0.9050 (0.8956)\t\n",
            "Epoch: [4][270/360]\tTime 0.017 (0.017)\tLoss 0.0467 (0.0390)\tPrec @1 0.8811 (0.8956)\t\n",
            "Epoch: [4][280/360]\tTime 0.017 (0.017)\tLoss 0.0573 (0.0389)\tPrec @1 0.8734 (0.8959)\t\n",
            "Epoch: [4][290/360]\tTime 0.016 (0.017)\tLoss 0.0328 (0.0388)\tPrec @1 0.9091 (0.8961)\t\n",
            "Epoch: [4][300/360]\tTime 0.016 (0.017)\tLoss 0.0244 (0.0386)\tPrec @1 0.9217 (0.8964)\t\n",
            "Epoch: [4][310/360]\tTime 0.017 (0.017)\tLoss 0.0488 (0.0384)\tPrec @1 0.8850 (0.8968)\t\n",
            "Epoch: [4][320/360]\tTime 0.016 (0.017)\tLoss 0.0185 (0.0382)\tPrec @1 0.9259 (0.8971)\t\n",
            "Epoch: [4][330/360]\tTime 0.017 (0.017)\tLoss 0.0525 (0.0382)\tPrec @1 0.8696 (0.8969)\t\n",
            "Epoch: [4][340/360]\tTime 0.017 (0.017)\tLoss 0.0221 (0.0382)\tPrec @1 0.9174 (0.8968)\t\n",
            "Epoch: [4][350/360]\tTime 0.017 (0.017)\tLoss 0.0397 (0.0385)\tPrec @1 0.8811 (0.8962)\t\n",
            "Epoch: [4][0/105]\tTime 0.026 (0.026)\t\n",
            "Epoch: [4][10/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [4][20/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [4][30/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [4][40/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [4][50/105]\tTime 0.028 (0.027)\t\n",
            "Epoch: [4][60/105]\tTime 0.031 (0.027)\t\n",
            "Epoch: [4][70/105]\tTime 0.026 (0.027)\t\n",
            "Epoch: [4][80/105]\tTime 0.027 (0.027)\t\n",
            "Epoch: [4][90/105]\tTime 0.025 (0.027)\t\n",
            "Epoch: [4][100/105]\tTime 0.033 (0.027)\t\n",
            "* Prec @1: 0.8923\n",
            " Best Prec @1 Acccuracy: 0.8927\n",
            " Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load saved model"
      ],
      "metadata": {
        "id": "1qKN9Ko7atu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "best_model = DecomposableAttention(100, 200).to(device)\n",
        "best_model.load_state_dict(torch.load('./checkpoints/decomposable_attention.pth'))\n",
        "best_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmm3V_GMZWpj",
        "outputId": "a5a21880-b6ea-4cdc-bee5-340b2acb9322"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecomposableAttention(\n",
              "  (attend): Attend(\n",
              "    (f): Sequential(\n",
              "      (0): Dropout(p=0.2, inplace=False)\n",
              "      (1): Linear(in_features=100, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.2, inplace=False)\n",
              "      (4): Linear(in_features=200, out_features=200, bias=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (compare): Compare(\n",
              "    (g): Sequential(\n",
              "      (0): Dropout(p=0.2, inplace=False)\n",
              "      (1): Linear(in_features=200, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.2, inplace=False)\n",
              "      (4): Linear(in_features=200, out_features=200, bias=True)\n",
              "      (5): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (aggregate): Aggregate(\n",
              "    (h): Sequential(\n",
              "      (0): Dropout(p=0.2, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Flatten(start_dim=1, end_dim=-1)\n",
              "      (4): Dropout(p=0.2, inplace=False)\n",
              "      (5): Linear(in_features=200, out_features=200, bias=True)\n",
              "      (6): ReLU()\n",
              "      (7): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (linear): Linear(in_features=200, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict on test dataset"
      ],
      "metadata": {
        "id": "aBCBAxjdppVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_neutral = 3"
      ],
      "metadata": {
        "id": "McltcuAM3VI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "data_train, ref_train, data_valid, ref_valid, data_test, ref_test = load()\n",
        "test_data=MyDataset(data_test, ref_test, use_faiss=False, max_neutral=3)\n",
        "test_loader=DataLoader(test_data,batch_size=len(test_data.df),\\\n",
        "                       collate_fn=coll, shuffle=False)\n",
        "best_model.eval()\n",
        "\n",
        "y_hat = None\n",
        "labels = None\n",
        "for idx, x in enumerate(test_loader):\n",
        "  with torch.no_grad():\n",
        "    out = best_model(x)\n",
        "    _, preds = torch.max(out, 1)   \n",
        "    #Get prediction and labels to get metrix\n",
        "    y_hat = preds.cpu().numpy()\n",
        "    labels = x[\"Label\"].cpu().numpy()\n",
        "    print(f\"\\n {classification_report(labels, y_hat, labels=[0,1,2])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5jZTJokb6Je",
        "outputId": "d49fdc90-b569-4a28-f748-b7cd2cba7693"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer/utility.py:131: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1971\n",
            "           1       0.00      0.00      0.00       422\n",
            "           2       0.89      1.00      0.94     19690\n",
            "\n",
            "    accuracy                           0.89     22083\n",
            "   macro avg       0.30      0.33      0.31     22083\n",
            "weighted avg       0.80      0.89      0.84     22083\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_neutral = 1"
      ],
      "metadata": {
        "id": "JPDiMMHA3ea0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "data_train, ref_train, data_valid, ref_valid, data_test, ref_test = load()\n",
        "test_data=MyDataset(data_test, ref_test, use_faiss=False, max_neutral=1)\n",
        "test_loader=DataLoader(test_data,batch_size=len(test_data.df),\\\n",
        "                       collate_fn=coll, shuffle=False)\n",
        "best_model.eval()\n",
        "\n",
        "y_hat = None\n",
        "labels = None\n",
        "for idx, x in enumerate(test_loader):\n",
        "  with torch.no_grad():\n",
        "    out = best_model(x)\n",
        "    _, preds = torch.max(out, 1)   \n",
        "    #Get prediction and labels to get metrix\n",
        "    y_hat = preds.cpu().numpy()\n",
        "    labels = x[\"Label\"].cpu().numpy()\n",
        "    print(f\"\\n {classification_report(labels, y_hat, labels=[0,1,2])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvBDxw0t3IlC",
        "outputId": "8fd4ead0-029f-4c81-ec78-8d76eaf3e9a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer/utility.py:131: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1971\n",
            "           1       0.00      0.00      0.00       422\n",
            "           2       0.89      1.00      0.94     19675\n",
            "\n",
            "    accuracy                           0.89     22068\n",
            "   macro avg       0.30      0.33      0.31     22068\n",
            "weighted avg       0.79      0.89      0.84     22068\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "max neutral = 0"
      ],
      "metadata": {
        "id": "8AQ83tp33ndU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "data_train, ref_train, data_valid, ref_valid, data_test, ref_test = load()\n",
        "test_data=MyDataset(data_test, ref_test, use_faiss=False, max_neutral=0)\n",
        "test_loader=DataLoader(test_data,batch_size=len(test_data.df),\\\n",
        "                       collate_fn=coll, shuffle=False)\n",
        "best_model.eval()\n",
        "\n",
        "y_hat = None\n",
        "labels = None\n",
        "for idx, x in enumerate(test_loader):\n",
        "  with torch.no_grad():\n",
        "    out = best_model(x)\n",
        "    _, preds = torch.max(out, 1)   \n",
        "    #Get prediction and labels to get metrix\n",
        "    y_hat = preds.cpu().numpy()\n",
        "    labels = x[\"Label\"].cpu().numpy()\n",
        "    print(f\"\\n {classification_report(labels, y_hat, labels=[0,1,2])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hWWpzbC3jxx",
        "outputId": "65465065-fa93-4808-c4fd-59a6c7ab7c53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Github/DeepLearning2022/ContractReviewer/utility.py:131: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1971\n",
            "           1       0.00      0.00      0.00       422\n",
            "           2       0.89      1.00      0.94     19751\n",
            "\n",
            "    accuracy                           0.89     22144\n",
            "   macro avg       0.30      0.33      0.31     22144\n",
            "weighted avg       0.80      0.89      0.84     22144\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}
