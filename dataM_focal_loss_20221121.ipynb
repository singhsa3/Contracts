{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**How this will work** <br>\n",
        "\n",
        "\n",
        "1.   A user will provide a hypothesis and a document and the model will then infer if there is any clause in the document that support, contradict or not mentioned(neutral) the hypothesis\n",
        "2.   Model will be batch trained. Each batch will represent one document and one hypothesis. \n",
        "> Note that in a document there can be 100's of clauses but just one or two clause may support or contradict the hypothesis. This implies over 90% of clauses will be neutral to the hypothesis. This makes data highly imbalanced <br>\n",
        "> I have a parameter max_neutral that controles how many neutral cases we want to add\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2vFmKXiVQOB"
      },
      "id": "N2vFmKXiVQOB"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXmxlsgVMgm",
        "outputId": "8605f2a1-ae86-4727-aecd-17d6a98e1c60"
      },
      "id": "1hXmxlsgVMgm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dir= \"/content/drive/MyDrive/Github/CS-7643/project/contract-nli\"\n",
        "os.chdir(dir)"
      ],
      "metadata": {
        "id": "gqTgjCZVqrg-"
      },
      "id": "gqTgjCZVqrg-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert-pytorch\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install onnx\n",
        "!pip install fasttext\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install d2l==1.0.0-alpha1.post0\n",
        "# https://huggingface.co/docs/transformers/preprocessing"
      ],
      "metadata": {
        "id": "-Zn1oKifzYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4e0058-e896-4f35-eb69-0f1673609383"
      },
      "id": "-Zn1oKifzYiQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 29.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 22.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.19.6)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3157913 sha256=7d0c7fcadb117822eb0bbcc16a830cdb9e241a51bfeec833371e25adaf4f054a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0-alpha1.post0\n",
            "  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.10.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.7.16)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.15.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.6)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n",
            "Successfully installed d2l-1.0.0a1.post0 jedi-0.18.2 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.4.0 qtpy-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re \n",
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchtext.data import get_tokenizer\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise"
      ],
      "metadata": {
        "id": "8-XJP1t2tF-y"
      },
      "id": "8-XJP1t2tF-y",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take time\n",
        "d=100\n",
        "from torchtext.vocab import GloVe\n",
        "#global_vectors = GloVe(name='840B', dim=d)\n",
        "global_vectors = GloVe(name='6B', dim=d)"
      ],
      "metadata": {
        "id": "t3mKE1z8GDQp"
      },
      "id": "t3mKE1z8GDQp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('omw-1.4')\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "mlngt = 100 #max_length,\n",
        "mn =10 #max_neutral\n",
        "\n",
        "def clean_head(ab):\n",
        "    ab = ab.replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\")\n",
        "    ab = ab.lower().strip()    \n",
        "    #marked_text = \"[CLS] \" + ab + \" [SEP]\"\n",
        "    # Tokenize our sentence with the BERT tokenizer.\n",
        "    #ab = tokenizer.tokenize(marked_text) \n",
        "    return ab\n",
        "\n",
        "def preprocess(sentence):\n",
        "  sentence=str(sentence)\n",
        "  sentence = sentence.lower().replace(\",\",\" ,\").replace(\";\",\" ;\").replace(\":\",\" :\").replace(\"__\",\" \").replace(\"  \",\" \")\n",
        "  sentence=sentence.replace('{html}',\"\") \n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', sentence)\n",
        "  rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "  rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(rem_num)  \n",
        "  #filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "  #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "  #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def getPositionEncoding(max_length, d, n=10000):\n",
        "    P = np.zeros((max_length, d))\n",
        "    #print(max_length,P.shape)\n",
        "    for k in range(max_length):      \n",
        "      for i in np.arange(int(d/2)):\n",
        "        denominator = np.power(n, 2*i/d)\n",
        "        P[k, 2*i] = np.sin(k/denominator)\n",
        "        P[k, 2*i+1] = np.cos(k/denominator)\n",
        "    return P\n",
        "Pg =  getPositionEncoding(mlngt, d)  \n",
        "Pg  = torch.FloatTensor(Pg)\n",
        "\n",
        "def emd(sent, max_length=mlngt):\n",
        "  tok = get_tokenizer(\"basic_english\")\n",
        "  tkn = tok(sent)  \n",
        "  ln = len(tkn)\n",
        "  x = global_vectors.get_vecs_by_tokens(tkn)\n",
        "  #print(x.shape, Pg.shape)\n",
        "  if ln<max_length:  \n",
        "    x = torch.nn.ConstantPad2d((0, 0, 0, max_length-ln), 0)(x)\n",
        "  else:\n",
        "    x= x[:max_length,:]\n",
        "  x=x+Pg \n",
        "  return x\n",
        "\n",
        "def emdb(sent):\n",
        "  # Tokenize sentences\n",
        "  encoded_input = tokenizer(sent, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "  # Compute token embeddings\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "\n",
        "  # Perform pooling. In this case, max pooling.\n",
        "  x = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "  seg = torch.tensor([0,1])\n",
        "  seg = seg.tile((768,1)).t()    \n",
        "  x=x+seg  \n",
        "  return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWPAon7tAWrj",
        "outputId": "e8ca6296-77c4-4365-bd97-65c7d95bb104"
      },
      "id": "mWPAon7tAWrj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ab = emd(\"we are the world\")\n",
        "ab.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llgdXTqnxxyE",
        "outputId": "14b96a34-abde-4010-c379-fd95ad7193a2"
      },
      "id": "llgdXTqnxxyE",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "# Opening JSON file\n",
        "f = open('train.json')  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n",
        "rg = len(data['documents'])\n",
        "reference_list=[]\n",
        "idx=0\n",
        "for idx in range(rg):\n",
        "  #Document level\n",
        "  docid = data['documents'][idx]['id']\n",
        "  for ndas_key in kes:\n",
        "    reference_list.append([docid, ndas_key,idx])\n",
        "  idx=idx+1\n",
        "ref_train = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])\n",
        "f.close()\n",
        "##############\n",
        "# Opening JSON file\n",
        "f = open('test.json')  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "dataT = json.load(f)\n",
        "\n",
        "kes=['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4']\n",
        "rg = len(dataT['documents'])\n",
        "reference_list=[]\n",
        "idxt=0\n",
        "for idxt in range(rg):\n",
        "  #Document level\n",
        "  docid = dataT['documents'][idxt]['id']\n",
        "\n",
        "  for ndas_key in kes:\n",
        "    reference_list.append([docid, ndas_key, idxt])\n",
        "  idxt=idxt+1\n",
        "ref_test = pd.DataFrame(reference_list ,columns = ['docid', 'nda_key', 'idx'])"
      ],
      "metadata": {
        "id": "wnzc05BuQWMy"
      },
      "id": "wnzc05BuQWMy",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Not delete\n",
        "def get_data(data, idx,ke, max_neutral=mn):\n",
        "  dataM=[]\n",
        "  #Document level\n",
        "  docid = data['documents'][idx]['id']\n",
        "  \n",
        "  string = data['documents'][idx]['text']\n",
        "  file_name = data['documents'][idx]['file_name']\n",
        "  spans = data['documents'][idx]['spans']\n",
        "  spanall=[]\n",
        "  for span in spans:           \n",
        "      spanval = string[span[0]: span[1]]\n",
        "      spanall.append(spanval)\n",
        "  \n",
        "  ndas = data['documents'][idx]['annotation_sets'][0]['annotations'] \n",
        "  # Keys level\n",
        "    \n",
        "  hypothesis = data['labels'][ke]['hypothesis']  \n",
        "\n",
        "  # Key level   \n",
        "\n",
        "  choice =ndas[ke] ['choice']\n",
        "  spansC = ndas[ke] ['spans']    \n",
        "  for si in range(len(spanall)):\n",
        "    span_nbr =si\n",
        "    if si in spansC:\n",
        "      val=choice\n",
        "    else:\n",
        "      val=\"Neutral\"\n",
        "    premise = spanall[si]\n",
        "    itm = [docid, file_name , ke, hypothesis, span_nbr, premise,val ]\n",
        "    dataM.append(itm)\n",
        "  df = pd.DataFrame(dataM ,columns = ['docid', 'file_name', 'hypotheis_key', 'hypotheis', 'span_nbr', 'premise', 'choice'   ])\n",
        "  #\"[CLS] \" and \" [SEP] \"\" [SEP]\" \n",
        "  df['premise'] = df['premise'].apply(preprocess)\n",
        "  df['hypotheis']= df['hypotheis'].apply(preprocess) \n",
        "  df['label'] = df['choice'].map(lambda s: 0 if s=='Entailment' else (1 if s== 'Contradiction' else 2 ))\n",
        "  df['entl'] =  df['choice'].map(lambda s : 1 if s== 'Entailment' else 0)\n",
        "  df['cont'] =  df['choice'].map(lambda s : 1 if s== 'Contradiction' else 0)\n",
        "  df['neut'] =  df['choice'].map(lambda s : 1 if s== 'Neutral' else 0)\n",
        "  \n",
        "  df1=df[df.choice !='Neutral']\n",
        "  df2=df[df.choice =='Neutral']\n",
        "  n = min(max_neutral, len(df2)-1)\n",
        "  df2= df2.sample(n = n)\n",
        "  df = pd.concat([df1, df2], sort=False)\n",
        "  df = df[df.span_nbr !=-1 ][df.premise != ''] [df.hypotheis != '']\n",
        "  #df['bert_sent']= df.apply(lambda s: [s.premise,s.hypotheis], axis=1)\n",
        "  return df\n",
        "\n",
        "def create_completedf(data, refdf, max_neutral=50):\n",
        "  cl = get_data(data, 0,'nda-1', 1)\n",
        "  cols = list(cl.columns)\n",
        "  df1= pd.DataFrame(columns=cols)\n",
        "  for ind, row in refdf.iterrows():    \n",
        "    df2 = get_data(data, row[\"idx\"],row[\"nda_key\"], max_neutral)\n",
        "    df1=pd.concat([df1,df2])\n",
        "  return df1\n",
        "\n"
      ],
      "metadata": {
        "id": "hYXSVq2As5R5"
      },
      "id": "hYXSVq2As5R5",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ref_test, ref_valid = train_test_split(ref_test, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "H2spD4CZ6Cbj"
      },
      "id": "H2spD4CZ6Cbj",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "DUzW-j3QyEJa"
      },
      "id": "DUzW-j3QyEJa",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset): \n",
        "  def __init__(self,data, df, max_neutral=mn):       \n",
        "    self.df =df.sample(frac=1)\n",
        "    self.data =data\n",
        "    self.max_neutral= max_neutral\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    # Is should have used docid but I am using index in json. Check how ref files are structured\n",
        "    docidx = self.df.iloc[idx]['idx']\n",
        "    ndakey = self.df.iloc[idx]['nda_key']\n",
        "    df1 = get_data(data, docidx, ndakey,self.max_neutral)\n",
        "    label = torch.tensor(df1.label.values)\n",
        "    premise = torch.stack(df1.premise.apply(emd).tolist())\n",
        "    hypothesis =  torch.stack(df1.hypotheis.apply(emd).tolist())\n",
        "    #bertsent  = torch.stack(df1.bert_sent.apply(emdb).tolist())\n",
        "    \n",
        "    #sample = {\"Label\": label.to(device), \"Premise\": premise.to(device) , \"Hypothesis\" : hypothesis.to(device), \"Bert_Sent\": bertsent.to(device)}\n",
        "    sample = {\"Label\": label.to(device), \"Premise\": premise.to(device) , \"Hypothesis\" : hypothesis.to(device)}\n",
        "    return sample\n",
        "\n",
        "# Custome collate function\n",
        "def coll(sample1):  \n",
        "  if len(sample1)<2:\n",
        "    return sample1\n",
        "  else:\n",
        "    label0 = sample1[0]['Label'].to(device)\n",
        "    premise0 = sample1[0]['Premise'].to(device)\n",
        "    hypothesis0 = sample1[0]['Hypothesis'].to(device)\n",
        "    #bertsent0 = sample1[0][\"Bert_Sent\"].to(device)\n",
        "    for i in range(1, len(sample1)):\n",
        "      labeli = sample1[i]['Label'].to(device)\n",
        "      premisei = sample1[i]['Premise'].to(device)\n",
        "      hypothesisi = sample1[i]['Hypothesis'].to(device)\n",
        "      #bertsenti = sample1[i][\"Bert_Sent\"].to(device)\n",
        "      label0 =torch.cat((label0,labeli))\n",
        "      premise0 = torch.cat((premise0, premisei))\n",
        "      hypothesis0 = torch.cat((hypothesis0,hypothesisi))\n",
        "      #bertsent0 = torch.cat((bertsent0, bertsenti))\n",
        "    #sample1 = {\"Label\": label0, \"Premise\": premise0 , \"Hypothesis\" : hypothesis0, \"Bert_Sent\": bertsent0}\n",
        "    sample1 = {\"Label\": label0, \"Premise\": premise0 , \"Hypothesis\" : hypothesis0}\n",
        "    return sample1\n",
        "    "
      ],
      "metadata": {
        "id": "qoI6bAQbFCgp"
      },
      "id": "qoI6bAQbFCgp",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=MyDataset(data, ref_train)\n",
        "train_loader=DataLoader(train_data,batch_size=20,collate_fn=coll, shuffle=False)\n",
        "valid_data=MyDataset(dataT, ref_valid)\n",
        "valid_loader=DataLoader(valid_data,batch_size=20,collate_fn=coll, shuffle=False)\n",
        "test_data=MyDataset(dataT, ref_test)\n",
        "test_loader=DataLoader(test_data,batch_size=20,collate_fn=coll, shuffle=False)\n"
      ],
      "metadata": {
        "id": "m-FgrJXnbTAb"
      },
      "id": "m-FgrJXnbTAb",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "xUTUoh3GcgPK"
      },
      "id": "xUTUoh3GcgPK",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(num_inputs, num_hiddens, flatten):\n",
        "    net = []\n",
        "    net.append(nn.Dropout(0.2))\n",
        "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
        "    net.append(nn.ReLU())\n",
        "    if flatten:\n",
        "        net.append(nn.Flatten(start_dim=1))\n",
        "    net.append(nn.Dropout(0.2))\n",
        "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
        "    net.append(nn.ReLU())\n",
        "    if flatten:\n",
        "        net.append(nn.Flatten(start_dim=1))\n",
        "    return nn.Sequential(*net)"
      ],
      "metadata": {
        "id": "0O1ZiCzfd0g4"
      },
      "id": "0O1ZiCzfd0g4",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attend(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
        "        super(Attend, self).__init__(**kwargs)\n",
        "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
        "\n",
        "    def forward(self, A, B):\n",
        "        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n",
        "        # `embed_size`)\n",
        "        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n",
        "        # `num_hiddens`)\n",
        "        f_A = self.f(A)\n",
        "        f_B = self.f(B)\n",
        "        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n",
        "        # no. of tokens in sequence B)\n",
        "        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
        "        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n",
        "        # `embed_size`), where sequence B is softly aligned with each token\n",
        "        # (axis 1 of `beta`) in sequence A\n",
        "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
        "        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n",
        "        # `embed_size`), where sequence A is softly aligned with each token\n",
        "        # (axis 1 of `alpha`) in sequence B\n",
        "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
        "        return beta, alpha"
      ],
      "metadata": {
        "id": "j_t0nss-d8KO"
      },
      "id": "j_t0nss-d8KO",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Compare(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
        "        super(Compare, self).__init__(**kwargs)\n",
        "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
        "\n",
        "    def forward(self, A, B, beta, alpha):\n",
        "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
        "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
        "        return V_A, V_B"
      ],
      "metadata": {
        "id": "64Iv0sxeeCA-"
      },
      "id": "64Iv0sxeeCA-",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Aggregate(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
        "        super(Aggregate, self).__init__(**kwargs)\n",
        "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
        "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
        "\n",
        "    def forward(self, V_A, V_B):\n",
        "        # Sum up both sets of comparison vectors\n",
        "        V_A = V_A.sum(dim=1)\n",
        "        V_B = V_B.sum(dim=1)\n",
        "        # Feed the concatenation of both summarization results into an MLP\n",
        "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
        "        return Y_hat"
      ],
      "metadata": {
        "id": "iqPMKNWaeF0v"
      },
      "id": "iqPMKNWaeF0v",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecomposableAttention(nn.Module):\n",
        "    def __init__(self,  embed_size, num_hiddens, num_inputs_attend=100,\n",
        "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
        "        super(DecomposableAttention, self).__init__(**kwargs)\n",
        "        #self.embedding = nn.Embedding(len(vocab), embed_size)\n",
        "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
        "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
        "        # There are 3 possible outputs: entailment, contradiction, and neutral\n",
        "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A, B = X['Premise'], X['Hypothesis']\n",
        "        if torch.cuda.is_available():\n",
        "          A = A.cuda()\n",
        "          B = B.cuda()\n",
        "\n",
        "        beta, alpha = self.attend(A, B)\n",
        "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
        "        Y_hat = self.aggregate(V_A, V_B)\n",
        "        return Y_hat"
      ],
      "metadata": {
        "id": "kmflPFlHeGzT"
      },
      "id": "kmflPFlHeGzT",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reweight(cls_num_list, beta=0.9999):\n",
        "    \"\"\"\n",
        "    Implement reweighting by effective numbers\n",
        "    :param cls_num_list: a list containing # of samples of each class\n",
        "    :param beta: hyper-parameter for reweighting, see paper for more details\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    per_cls_weights = None\n",
        "    #############################################################################\n",
        "    # TODO: reweight each class by effective numbers                            #\n",
        "    #############################################################################\n",
        "    per_cls_weights = []\n",
        "    for cls_num in cls_num_list:\n",
        "        per_cls_weights.append((1 - beta) / (1 - (beta ** cls_num)))         \n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "    return per_cls_weights\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=0.):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        assert gamma >= 0\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        Implement forward of focal loss\n",
        "        :param input: input predictions\n",
        "        :param target: labels\n",
        "        :return: tensor of focal loss in scalar\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        #############################################################################\n",
        "        # TODO: Implement forward pass of the focal loss                            #\n",
        "        #############################################################################\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        import torch.nn.functional as F\n",
        "        weight=torch.from_numpy(np.array(self.weight)).float().to(device)\n",
        "        ce_loss = F.cross_entropy(input, target,  weight=weight)\n",
        "        pt = torch.exp(-ce_loss) \n",
        "        foc_loss = ((1 -pt) ** self.gamma * ce_loss)\n",
        "        loss = foc_loss.mean()\n",
        "        #############################################################################\n",
        "        #                              END OF YOUR CODE                             #\n",
        "        #############################################################################\n",
        "        return loss"
      ],
      "metadata": {
        "id": "JXK19zN3AVZR"
      },
      "id": "JXK19zN3AVZR",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "#                             Counts for focal loss                         #\n",
        "#############################################################################\n",
        "entailment = 0\n",
        "contradiction = 0\n",
        "neutral = 0\n",
        "\n",
        "for i, x in enumerate(train_loader):\n",
        "  a = x['Label'].bincount().cpu().numpy()\n",
        "  entailment += a[0]\n",
        "  contradiction += a[1]\n",
        "  neutral += a[2]\n",
        "\n",
        "cls_num_list = list([entailment,contradiction,neutral])\n",
        "\n",
        "#############################################################################\n",
        "#                              END OF YOUR CODE                             #\n",
        "#############################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teDTojd4mZsP",
        "outputId": "414a7653-f4bc-4425-f784-e80886ee8882"
      },
      "id": "teDTojd4mZsP",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_num_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DR6jJOqra8e",
        "outputId": "c7c52574-72ab-4186-ae7a-06eb74d1f0db"
      },
      "id": "-DR6jJOqra8e",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6759, 1578, 67424]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_cls_weights = reweight(list([6759,1578, 67465]), beta=.9999)"
      ],
      "metadata": {
        "id": "3E3Evrp7sKwv"
      },
      "id": "3E3Evrp7sKwv",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "net = DecomposableAttention(100, 200).to(device)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = FocalLoss(weight=per_cls_weights, gamma=1).to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    print(f\"running epoch {epoch}\")\n",
        "    for i, x in enumerate(train_loader):       \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #print(f\"running i {i}\")\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net.forward(x)\n",
        "        loss = criterion(outputs, x[\"Label\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgowrGX-0Tsv",
        "outputId": "34c378c8-5b20-4731-93d3-ba9956e5294c"
      },
      "id": "bgowrGX-0Tsv",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   360] loss: 14.119\n",
            "running epoch 1\n",
            "[2,   360] loss: 11.858\n",
            "running epoch 2\n",
            "[3,   360] loss: 10.798\n",
            "running epoch 3\n",
            "[4,   360] loss: 9.978\n",
            "running epoch 4\n",
            "[5,   360] loss: 9.674\n",
            "running epoch 5\n",
            "[6,   360] loss: 9.464\n",
            "running epoch 6\n",
            "[7,   360] loss: 9.190\n",
            "running epoch 7\n",
            "[8,   360] loss: 9.018\n",
            "running epoch 8\n",
            "[9,   360] loss: 8.770\n",
            "running epoch 9\n",
            "[10,   360] loss: 8.682\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, x in enumerate(test_loader):\n",
        "        labels = x[\"Label\"]\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        print(f\"looping over item {i}\")\n",
        "        \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test documents: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "tDST8lCS7mf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f69f633-ddc8-4680-83fa-8e8a172f4c09"
      },
      "id": "tDST8lCS7mf1",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looping over item 0\n",
            "looping over item 1\n",
            "looping over item 2\n",
            "looping over item 3\n",
            "looping over item 4\n",
            "looping over item 5\n",
            "looping over item 6\n",
            "looping over item 7\n",
            "looping over item 8\n",
            "looping over item 9\n",
            "looping over item 10\n",
            "looping over item 11\n",
            "looping over item 12\n",
            "looping over item 13\n",
            "looping over item 14\n",
            "looping over item 15\n",
            "looping over item 16\n",
            "looping over item 17\n",
            "looping over item 18\n",
            "looping over item 19\n",
            "looping over item 20\n",
            "looping over item 21\n",
            "looping over item 22\n",
            "looping over item 23\n",
            "looping over item 24\n",
            "looping over item 25\n",
            "looping over item 26\n",
            "looping over item 27\n",
            "looping over item 28\n",
            "looping over item 29\n",
            "looping over item 30\n",
            "looping over item 31\n",
            "looping over item 32\n",
            "looping over item 33\n",
            "looping over item 34\n",
            "looping over item 35\n",
            "looping over item 36\n",
            "looping over item 37\n",
            "looping over item 38\n",
            "looping over item 39\n",
            "looping over item 40\n",
            "looping over item 41\n",
            "looping over item 42\n",
            "looping over item 43\n",
            "looping over item 44\n",
            "looping over item 45\n",
            "looping over item 46\n",
            "looping over item 47\n",
            "looping over item 48\n",
            "looping over item 49\n",
            "looping over item 50\n",
            "looping over item 51\n",
            "looping over item 52\n",
            "Accuracy of the network on the 10000 test documents: 86 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**________________ END--------------------------**"
      ],
      "metadata": {
        "id": "xBYsyZYJ45G-"
      },
      "id": "xBYsyZYJ45G-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}